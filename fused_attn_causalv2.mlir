module @module {
  util.global private @__auto.constant_1_1_4096_4096_torch.uint8 = dense_resource<__auto.constant_1_1_4096_4096_torch.uint8> : tensor<1x1x4096x4096xi8>
  func.func @prefill_bs4(%arg0: !torch.vtensor<[4,64,32,128],f16>, %arg1: !torch.vtensor<[4,64,32,128],f16>, %arg2: !torch.vtensor<[4,64,32,128],f16>, %arg3: !torch.vtensor<[4],si64>, %arg4: !torch.vtensor<[4,4],si64>, %arg5: !torch.tensor<[256,4194304],f16>) -> !torch.vtensor<[4,64,4096],f16> attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.constant_1_1_4096_4096_torch.uint8 = util.global.load @__auto.constant_1_1_4096_4096_torch.uint8 : tensor<1x1x4096x4096xi8>
    %0 = torch_c.from_builtin_tensor %__auto.constant_1_1_4096_4096_torch.uint8 : tensor<1x1x4096x4096xi8> -> !torch.vtensor<[1,1,4096,4096],ui8>
    %int11 = torch.constant.int 11
    %1 = torch.prims.convert_element_type %0, %int11 : !torch.vtensor<[1,1,4096,4096],ui8>, !torch.int -> !torch.vtensor<[1,1,4096,4096],i1>
    %2 = torch.copy.to_vtensor %arg5 : !torch.vtensor<[256,4194304],f16>
    %int-1 = torch.constant.int -1
    %int32 = torch.constant.int 32
    %int2 = torch.constant.int 2
    %int16 = torch.constant.int 16
    %int32_0 = torch.constant.int 32
    %int128 = torch.constant.int 128
    %3 = torch.prim.ListConstruct %int-1, %int32, %int2, %int16, %int32_0, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4 = torch.aten.view %2, %3 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
    %int16384 = torch.constant.int 16384
    %int16_1 = torch.constant.int 16
    %int32_2 = torch.constant.int 32
    %int128_3 = torch.constant.int 128
    %5 = torch.prim.ListConstruct %int16384, %int16_1, %int32_2, %int128_3 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6 = torch.aten.view %4, %5 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[16384,16,32,128],f16>
    %int64 = torch.constant.int 64
    %7 = torch.aten.mul.Scalar %arg4, %int64 : !torch.vtensor<[4,4],si64>, !torch.int -> !torch.vtensor<[4,4],si64>
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %8 = torch.aten.add.Scalar %7, %int0, %int1 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,4],si64>
    %int4 = torch.constant.int 4
    %int4_4 = torch.constant.int 4
    %int16_5 = torch.constant.int 16
    %int32_6 = torch.constant.int 32
    %int128_7 = torch.constant.int 128
    %9 = torch.prim.ListConstruct %int4, %int4_4, %int16_5, %int32_6, %int128_7 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10 = torch.aten.view %arg1, %9 : !torch.vtensor<[4,64,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,4,16,32,128],f16>
    %int16_8 = torch.constant.int 16
    %11 = torch.prim.ListConstruct %int16_8 : (!torch.int) -> !torch.list<int>
    %12 = torch.aten.view %8, %11 : !torch.vtensor<[4,4],si64>, !torch.list<int> -> !torch.vtensor<[16],si64>
    %int16_9 = torch.constant.int 16
    %int16_10 = torch.constant.int 16
    %int32_11 = torch.constant.int 32
    %int128_12 = torch.constant.int 128
    %13 = torch.prim.ListConstruct %int16_9, %int16_10, %int32_11, %int128_12 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14 = torch.aten.view %10, %13 : !torch.vtensor<[4,4,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[16,16,32,128],f16>
    %15 = torch.prim.ListConstruct %12 : (!torch.vtensor<[16],si64>) -> !torch.list<optional<vtensor>>
    %false = torch.constant.bool false
    %16 = torch.aten.index_put %6, %15, %14, %false : !torch.vtensor<[16384,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[16,16,32,128],f16>, !torch.bool -> !torch.vtensor<[16384,16,32,128],f16>
    %int256 = torch.constant.int 256
    %int32_13 = torch.constant.int 32
    %int2_14 = torch.constant.int 2
    %int16_15 = torch.constant.int 16
    %int32_16 = torch.constant.int 32
    %int128_17 = torch.constant.int 128
    %17 = torch.prim.ListConstruct %int256, %int32_13, %int2_14, %int16_15, %int32_16, %int128_17 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %18 = torch.aten.view %16, %17 : !torch.vtensor<[16384,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
    %int256_18 = torch.constant.int 256
    %int4194304 = torch.constant.int 4194304
    %19 = torch.prim.ListConstruct %int256_18, %int4194304 : (!torch.int, !torch.int) -> !torch.list<int>
    %20 = torch.aten.view %18, %19 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
    %int-1_19 = torch.constant.int -1
    %int32_20 = torch.constant.int 32
    %int2_21 = torch.constant.int 2
    %int16_22 = torch.constant.int 16
    %int32_23 = torch.constant.int 32
    %int128_24 = torch.constant.int 128
    %21 = torch.prim.ListConstruct %int-1_19, %int32_20, %int2_21, %int16_22, %int32_23, %int128_24 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %22 = torch.aten.view %20, %21 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
    %int16384_25 = torch.constant.int 16384
    %int16_26 = torch.constant.int 16
    %int32_27 = torch.constant.int 32
    %int128_28 = torch.constant.int 128
    %23 = torch.prim.ListConstruct %int16384_25, %int16_26, %int32_27, %int128_28 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %24 = torch.aten.view %22, %23 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[16384,16,32,128],f16>
    %int4_29 = torch.constant.int 4
    %int4_30 = torch.constant.int 4
    %int16_31 = torch.constant.int 16
    %int32_32 = torch.constant.int 32
    %int128_33 = torch.constant.int 128
    %25 = torch.prim.ListConstruct %int4_29, %int4_30, %int16_31, %int32_32, %int128_33 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %26 = torch.aten.view %arg2, %25 : !torch.vtensor<[4,64,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,4,16,32,128],f16>
    %int1_34 = torch.constant.int 1
    %int1_35 = torch.constant.int 1
    %27 = torch.aten.add.Scalar %8, %int1_34, %int1_35 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,4],si64>
    %int16_36 = torch.constant.int 16
    %28 = torch.prim.ListConstruct %int16_36 : (!torch.int) -> !torch.list<int>
    %29 = torch.aten.view %27, %28 : !torch.vtensor<[4,4],si64>, !torch.list<int> -> !torch.vtensor<[16],si64>
    %int16_37 = torch.constant.int 16
    %int16_38 = torch.constant.int 16
    %int32_39 = torch.constant.int 32
    %int128_40 = torch.constant.int 128
    %30 = torch.prim.ListConstruct %int16_37, %int16_38, %int32_39, %int128_40 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %31 = torch.aten.view %26, %30 : !torch.vtensor<[4,4,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[16,16,32,128],f16>
    %32 = torch.prim.ListConstruct %29 : (!torch.vtensor<[16],si64>) -> !torch.list<optional<vtensor>>
    %false_41 = torch.constant.bool false
    %33 = torch.aten.index_put %24, %32, %31, %false_41 : !torch.vtensor<[16384,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[16,16,32,128],f16>, !torch.bool -> !torch.vtensor<[16384,16,32,128],f16>
    %int256_42 = torch.constant.int 256
    %int32_43 = torch.constant.int 32
    %int2_44 = torch.constant.int 2
    %int16_45 = torch.constant.int 16
    %int32_46 = torch.constant.int 32
    %int128_47 = torch.constant.int 128
    %34 = torch.prim.ListConstruct %int256_42, %int32_43, %int2_44, %int16_45, %int32_46, %int128_47 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %35 = torch.aten.view %33, %34 : !torch.vtensor<[16384,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
    %int256_48 = torch.constant.int 256
    %int4194304_49 = torch.constant.int 4194304
    %36 = torch.prim.ListConstruct %int256_48, %int4194304_49 : (!torch.int, !torch.int) -> !torch.list<int>
    %37 = torch.aten.view %35, %36 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
    torch.overwrite.tensor.contents %37 overwrites %arg5 : !torch.vtensor<[256,4194304],f16>, !torch.tensor<[256,4194304],f16>
    %int1_50 = torch.constant.int 1
    %int2_51 = torch.constant.int 2
    %38 = torch.aten.transpose.int %arg0, %int1_50, %int2_51 : !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,64,128],f16>
    %int1_52 = torch.constant.int 1
    %int2_53 = torch.constant.int 2
    %39 = torch.aten.transpose.int %arg1, %int1_52, %int2_53 : !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,64,128],f16>
    %int1_54 = torch.constant.int 1
    %int2_55 = torch.constant.int 2
    %40 = torch.aten.transpose.int %arg2, %int1_54, %int2_55 : !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,64,128],f16>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %true = torch.constant.bool true
    %none = torch.constant.none
    %none_56 = torch.constant.none
    %41:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%38, %39, %40, %float0.000000e00, %true, %none, %none_56) : (!torch.vtensor<[4,32,64,128],f16>, !torch.vtensor<[4,32,64,128],f16>, !torch.vtensor<[4,32,64,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,64,128],f16>, !torch.vtensor<[4,32,64],f32>) 
    %int1_57 = torch.constant.int 1
    %int2_58 = torch.constant.int 2
    %42 = torch.aten.transpose.int %41#0, %int1_57, %int2_58 : !torch.vtensor<[4,32,64,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,64,32,128],f16>
    %int4_59 = torch.constant.int 4
    %int64_60 = torch.constant.int 64
    %int-1_61 = torch.constant.int -1
    %43 = torch.prim.ListConstruct %int4_59, %int64_60, %int-1_61 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %44 = torch.aten.view %42, %43 : !torch.vtensor<[4,64,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,64,4096],f16>
    return %44 : !torch.vtensor<[4,64,4096],f16>
  }
  // func.func @decode_bs4(%arg0: !torch.vtensor<[4,1,32,128],f16>, %arg1: !torch.vtensor<[4,1,32,128],f16>, %arg2: !torch.vtensor<[4,1,32,128],f16>, %arg3: !torch.vtensor<[4],si64>, %arg4: !torch.vtensor<[4],si64>, %arg5: !torch.vtensor<[4,4],si64>, %arg6: !torch.tensor<[256,4194304],f16>) -> !torch.vtensor<[4,1,4096],f16> attributes {torch.assume_strict_symbolic_shapes} {
  //   %0 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %1 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %2 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %3 = torch.vtensor.literal(dense<1> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %4 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %5 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %6 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %7 = torch.vtensor.literal(dense<1> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %8 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %9 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %10 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %11 = torch.vtensor.literal(dense<1> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %12 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %13 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %14 = torch.vtensor.literal(dense<0> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %15 = torch.vtensor.literal(dense<1> : tensor<1xsi64>) : !torch.vtensor<[1],si64>
  //   %16 = torch.copy.to_vtensor %arg6 : !torch.vtensor<[256,4194304],f16>
  //   %int4 = torch.constant.int 4
  //   %int4096 = torch.constant.int 4096
  //   %int32 = torch.constant.int 32
  //   %int128 = torch.constant.int 128
  //   %17 = torch.prim.ListConstruct %int4, %int4096, %int32, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %int5 = torch.constant.int 5
  //   %none = torch.constant.none
  //   %cpu = torch.constant.device "cpu"
  //   %false = torch.constant.bool false
  //   %none_0 = torch.constant.none
  //   %18 = torch.aten.empty.memory_format %17, %int5, %none, %cpu, %false, %none_0 : !torch.list<int>, !torch.int, !torch.none, !torch.Device, !torch.bool, !torch.none -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int4_1 = torch.constant.int 4
  //   %int4096_2 = torch.constant.int 4096
  //   %int32_3 = torch.constant.int 32
  //   %int128_4 = torch.constant.int 128
  //   %19 = torch.prim.ListConstruct %int4_1, %int4096_2, %int32_3, %int128_4 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %int5_5 = torch.constant.int 5
  //   %none_6 = torch.constant.none
  //   %cpu_7 = torch.constant.device "cpu"
  //   %false_8 = torch.constant.bool false
  //   %none_9 = torch.constant.none
  //   %20 = torch.aten.empty.memory_format %19, %int5_5, %none_6, %cpu_7, %false_8, %none_9 : !torch.list<int>, !torch.int, !torch.none, !torch.Device, !torch.bool, !torch.none -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1 = torch.constant.int 1
  //   %int1_10 = torch.constant.int 1
  //   %21 = torch.aten.add.Scalar %arg4, %int1, %int1_10 : !torch.vtensor<[4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int-1 = torch.constant.int -1
  //   %int32_11 = torch.constant.int 32
  //   %int2 = torch.constant.int 2
  //   %int16 = torch.constant.int 16
  //   %int32_12 = torch.constant.int 32
  //   %int128_13 = torch.constant.int 128
  //   %22 = torch.prim.ListConstruct %int-1, %int32_11, %int2, %int16, %int32_12, %int128_13 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %23 = torch.aten.view %16, %22 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int0 = torch.constant.int 0
  //   %int0_14 = torch.constant.int 0
  //   %24 = torch.aten.select.int %21, %int0, %int0_14 : !torch.vtensor<[4],si64>, !torch.int, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_15 = torch.constant.int 0
  //   %int0_16 = torch.constant.int 0
  //   %25 = torch.aten.select.int %arg5, %int0_15, %int0_16 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int0_17 = torch.constant.int 0
  //   %int0_18 = torch.constant.int 0
  //   %int9223372036854775807 = torch.constant.int 9223372036854775807
  //   %int1_19 = torch.constant.int 1
  //   %26 = torch.aten.slice.Tensor %25, %int0_17, %int0_18, %int9223372036854775807, %int1_19 : !torch.vtensor<[4],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int16_20 = torch.constant.int 16
  //   %27 = torch.aten.floor_divide.Scalar %24, %int16_20 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_21 = torch.constant.int 0
  //   %28 = torch.aten.index_select %26, %int0_21, %27 : !torch.vtensor<[4],si64>, !torch.int, !torch.vtensor<[],si64> -> !torch.vtensor<[1],si64>
  //   %int16_22 = torch.constant.int 16
  //   %29 = torch.aten.remainder.Scalar %24, %int16_22 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %none_23 = torch.constant.none
  //   %30 = torch.aten.clone %0, %none_23 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_24 = torch.constant.none
  //   %31 = torch.aten.clone %1, %none_24 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_25 = torch.constant.int 0
  //   %32 = torch.aten.unsqueeze %29, %int0_25 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_26 = torch.constant.int 0
  //   %int0_27 = torch.constant.int 0
  //   %33 = torch.aten.select.int %arg1, %int0_26, %int0_27 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_28 = torch.constant.int 0
  //   %int0_29 = torch.constant.int 0
  //   %34 = torch.aten.select.int %33, %int0_28, %int0_29 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %35 = torch.prim.ListConstruct %28, %30, %31, %32 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_30 = torch.constant.bool false
  //   %36 = torch.aten.index_put %23, %35, %34, %false_30 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256 = torch.constant.int 256
  //   %int4194304 = torch.constant.int 4194304
  //   %37 = torch.prim.ListConstruct %int256, %int4194304 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %38 = torch.aten.view %36, %37 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_31 = torch.constant.int -1
  //   %int32_32 = torch.constant.int 32
  //   %int2_33 = torch.constant.int 2
  //   %int16_34 = torch.constant.int 16
  //   %int32_35 = torch.constant.int 32
  //   %int128_36 = torch.constant.int 128
  //   %39 = torch.prim.ListConstruct %int-1_31, %int32_32, %int2_33, %int16_34, %int32_35, %int128_36 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %40 = torch.aten.view %38, %39 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %none_37 = torch.constant.none
  //   %41 = torch.aten.clone %2, %none_37 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_38 = torch.constant.none
  //   %42 = torch.aten.clone %3, %none_38 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_39 = torch.constant.int 0
  //   %43 = torch.aten.unsqueeze %29, %int0_39 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_40 = torch.constant.int 0
  //   %int0_41 = torch.constant.int 0
  //   %44 = torch.aten.select.int %arg2, %int0_40, %int0_41 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_42 = torch.constant.int 0
  //   %int0_43 = torch.constant.int 0
  //   %45 = torch.aten.select.int %44, %int0_42, %int0_43 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %46 = torch.prim.ListConstruct %28, %41, %42, %43 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_44 = torch.constant.bool false
  //   %47 = torch.aten.index_put %40, %46, %45, %false_44 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_45 = torch.constant.int 256
  //   %int4194304_46 = torch.constant.int 4194304
  //   %48 = torch.prim.ListConstruct %int256_45, %int4194304_46 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %49 = torch.aten.view %47, %48 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_47 = torch.constant.int -1
  //   %int32_48 = torch.constant.int 32
  //   %int2_49 = torch.constant.int 2
  //   %int16_50 = torch.constant.int 16
  //   %int32_51 = torch.constant.int 32
  //   %int128_52 = torch.constant.int 128
  //   %50 = torch.prim.ListConstruct %int-1_47, %int32_48, %int2_49, %int16_50, %int32_51, %int128_52 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %51 = torch.aten.view %49, %50 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int0_53 = torch.constant.int 0
  //   %int1_54 = torch.constant.int 1
  //   %52 = torch.aten.select.int %21, %int0_53, %int1_54 : !torch.vtensor<[4],si64>, !torch.int, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_55 = torch.constant.int 0
  //   %int1_56 = torch.constant.int 1
  //   %53 = torch.aten.select.int %arg5, %int0_55, %int1_56 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int0_57 = torch.constant.int 0
  //   %int0_58 = torch.constant.int 0
  //   %int9223372036854775807_59 = torch.constant.int 9223372036854775807
  //   %int1_60 = torch.constant.int 1
  //   %54 = torch.aten.slice.Tensor %53, %int0_57, %int0_58, %int9223372036854775807_59, %int1_60 : !torch.vtensor<[4],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int16_61 = torch.constant.int 16
  //   %55 = torch.aten.floor_divide.Scalar %52, %int16_61 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_62 = torch.constant.int 0
  //   %56 = torch.aten.index_select %54, %int0_62, %55 : !torch.vtensor<[4],si64>, !torch.int, !torch.vtensor<[],si64> -> !torch.vtensor<[1],si64>
  //   %int16_63 = torch.constant.int 16
  //   %57 = torch.aten.remainder.Scalar %52, %int16_63 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %none_64 = torch.constant.none
  //   %58 = torch.aten.clone %4, %none_64 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_65 = torch.constant.none
  //   %59 = torch.aten.clone %5, %none_65 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_66 = torch.constant.int 0
  //   %60 = torch.aten.unsqueeze %57, %int0_66 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_67 = torch.constant.int 0
  //   %int1_68 = torch.constant.int 1
  //   %61 = torch.aten.select.int %arg1, %int0_67, %int1_68 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_69 = torch.constant.int 0
  //   %int0_70 = torch.constant.int 0
  //   %62 = torch.aten.select.int %61, %int0_69, %int0_70 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %63 = torch.prim.ListConstruct %56, %58, %59, %60 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_71 = torch.constant.bool false
  //   %64 = torch.aten.index_put %51, %63, %62, %false_71 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_72 = torch.constant.int 256
  //   %int4194304_73 = torch.constant.int 4194304
  //   %65 = torch.prim.ListConstruct %int256_72, %int4194304_73 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %66 = torch.aten.view %64, %65 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_74 = torch.constant.int -1
  //   %int32_75 = torch.constant.int 32
  //   %int2_76 = torch.constant.int 2
  //   %int16_77 = torch.constant.int 16
  //   %int32_78 = torch.constant.int 32
  //   %int128_79 = torch.constant.int 128
  //   %67 = torch.prim.ListConstruct %int-1_74, %int32_75, %int2_76, %int16_77, %int32_78, %int128_79 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %68 = torch.aten.view %66, %67 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %none_80 = torch.constant.none
  //   %69 = torch.aten.clone %6, %none_80 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_81 = torch.constant.none
  //   %70 = torch.aten.clone %7, %none_81 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_82 = torch.constant.int 0
  //   %71 = torch.aten.unsqueeze %57, %int0_82 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_83 = torch.constant.int 0
  //   %int1_84 = torch.constant.int 1
  //   %72 = torch.aten.select.int %arg2, %int0_83, %int1_84 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_85 = torch.constant.int 0
  //   %int0_86 = torch.constant.int 0
  //   %73 = torch.aten.select.int %72, %int0_85, %int0_86 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %74 = torch.prim.ListConstruct %56, %69, %70, %71 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_87 = torch.constant.bool false
  //   %75 = torch.aten.index_put %68, %74, %73, %false_87 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_88 = torch.constant.int 256
  //   %int4194304_89 = torch.constant.int 4194304
  //   %76 = torch.prim.ListConstruct %int256_88, %int4194304_89 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %77 = torch.aten.view %75, %76 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_90 = torch.constant.int -1
  //   %int32_91 = torch.constant.int 32
  //   %int2_92 = torch.constant.int 2
  //   %int16_93 = torch.constant.int 16
  //   %int32_94 = torch.constant.int 32
  //   %int128_95 = torch.constant.int 128
  //   %78 = torch.prim.ListConstruct %int-1_90, %int32_91, %int2_92, %int16_93, %int32_94, %int128_95 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %79 = torch.aten.view %77, %78 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int0_96 = torch.constant.int 0
  //   %int2_97 = torch.constant.int 2
  //   %80 = torch.aten.select.int %21, %int0_96, %int2_97 : !torch.vtensor<[4],si64>, !torch.int, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_98 = torch.constant.int 0
  //   %int2_99 = torch.constant.int 2
  //   %81 = torch.aten.select.int %arg5, %int0_98, %int2_99 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int0_100 = torch.constant.int 0
  //   %int0_101 = torch.constant.int 0
  //   %int9223372036854775807_102 = torch.constant.int 9223372036854775807
  //   %int1_103 = torch.constant.int 1
  //   %82 = torch.aten.slice.Tensor %81, %int0_100, %int0_101, %int9223372036854775807_102, %int1_103 : !torch.vtensor<[4],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int16_104 = torch.constant.int 16
  //   %83 = torch.aten.floor_divide.Scalar %80, %int16_104 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_105 = torch.constant.int 0
  //   %84 = torch.aten.index_select %82, %int0_105, %83 : !torch.vtensor<[4],si64>, !torch.int, !torch.vtensor<[],si64> -> !torch.vtensor<[1],si64>
  //   %int16_106 = torch.constant.int 16
  //   %85 = torch.aten.remainder.Scalar %80, %int16_106 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %none_107 = torch.constant.none
  //   %86 = torch.aten.clone %8, %none_107 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_108 = torch.constant.none
  //   %87 = torch.aten.clone %9, %none_108 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_109 = torch.constant.int 0
  //   %88 = torch.aten.unsqueeze %85, %int0_109 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_110 = torch.constant.int 0
  //   %int2_111 = torch.constant.int 2
  //   %89 = torch.aten.select.int %arg1, %int0_110, %int2_111 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_112 = torch.constant.int 0
  //   %int0_113 = torch.constant.int 0
  //   %90 = torch.aten.select.int %89, %int0_112, %int0_113 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %91 = torch.prim.ListConstruct %84, %86, %87, %88 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_114 = torch.constant.bool false
  //   %92 = torch.aten.index_put %79, %91, %90, %false_114 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_115 = torch.constant.int 256
  //   %int4194304_116 = torch.constant.int 4194304
  //   %93 = torch.prim.ListConstruct %int256_115, %int4194304_116 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %94 = torch.aten.view %92, %93 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_117 = torch.constant.int -1
  //   %int32_118 = torch.constant.int 32
  //   %int2_119 = torch.constant.int 2
  //   %int16_120 = torch.constant.int 16
  //   %int32_121 = torch.constant.int 32
  //   %int128_122 = torch.constant.int 128
  //   %95 = torch.prim.ListConstruct %int-1_117, %int32_118, %int2_119, %int16_120, %int32_121, %int128_122 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %96 = torch.aten.view %94, %95 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %none_123 = torch.constant.none
  //   %97 = torch.aten.clone %10, %none_123 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_124 = torch.constant.none
  //   %98 = torch.aten.clone %11, %none_124 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_125 = torch.constant.int 0
  //   %99 = torch.aten.unsqueeze %85, %int0_125 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_126 = torch.constant.int 0
  //   %int2_127 = torch.constant.int 2
  //   %100 = torch.aten.select.int %arg2, %int0_126, %int2_127 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_128 = torch.constant.int 0
  //   %int0_129 = torch.constant.int 0
  //   %101 = torch.aten.select.int %100, %int0_128, %int0_129 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %102 = torch.prim.ListConstruct %84, %97, %98, %99 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_130 = torch.constant.bool false
  //   %103 = torch.aten.index_put %96, %102, %101, %false_130 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_131 = torch.constant.int 256
  //   %int4194304_132 = torch.constant.int 4194304
  //   %104 = torch.prim.ListConstruct %int256_131, %int4194304_132 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %105 = torch.aten.view %103, %104 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_133 = torch.constant.int -1
  //   %int32_134 = torch.constant.int 32
  //   %int2_135 = torch.constant.int 2
  //   %int16_136 = torch.constant.int 16
  //   %int32_137 = torch.constant.int 32
  //   %int128_138 = torch.constant.int 128
  //   %106 = torch.prim.ListConstruct %int-1_133, %int32_134, %int2_135, %int16_136, %int32_137, %int128_138 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %107 = torch.aten.view %105, %106 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int0_139 = torch.constant.int 0
  //   %int3 = torch.constant.int 3
  //   %108 = torch.aten.select.int %21, %int0_139, %int3 : !torch.vtensor<[4],si64>, !torch.int, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_140 = torch.constant.int 0
  //   %int3_141 = torch.constant.int 3
  //   %109 = torch.aten.select.int %arg5, %int0_140, %int3_141 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int0_142 = torch.constant.int 0
  //   %int0_143 = torch.constant.int 0
  //   %int9223372036854775807_144 = torch.constant.int 9223372036854775807
  //   %int1_145 = torch.constant.int 1
  //   %110 = torch.aten.slice.Tensor %109, %int0_142, %int0_143, %int9223372036854775807_144, %int1_145 : !torch.vtensor<[4],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4],si64>
  //   %int16_146 = torch.constant.int 16
  //   %111 = torch.aten.floor_divide.Scalar %108, %int16_146 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %int0_147 = torch.constant.int 0
  //   %112 = torch.aten.index_select %110, %int0_147, %111 : !torch.vtensor<[4],si64>, !torch.int, !torch.vtensor<[],si64> -> !torch.vtensor<[1],si64>
  //   %int16_148 = torch.constant.int 16
  //   %113 = torch.aten.remainder.Scalar %108, %int16_148 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[],si64>
  //   %none_149 = torch.constant.none
  //   %114 = torch.aten.clone %12, %none_149 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_150 = torch.constant.none
  //   %115 = torch.aten.clone %13, %none_150 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_151 = torch.constant.int 0
  //   %116 = torch.aten.unsqueeze %113, %int0_151 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_152 = torch.constant.int 0
  //   %int3_153 = torch.constant.int 3
  //   %117 = torch.aten.select.int %arg1, %int0_152, %int3_153 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_154 = torch.constant.int 0
  //   %int0_155 = torch.constant.int 0
  //   %118 = torch.aten.select.int %117, %int0_154, %int0_155 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %119 = torch.prim.ListConstruct %112, %114, %115, %116 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_156 = torch.constant.bool false
  //   %120 = torch.aten.index_put %107, %119, %118, %false_156 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_157 = torch.constant.int 256
  //   %int4194304_158 = torch.constant.int 4194304
  //   %121 = torch.prim.ListConstruct %int256_157, %int4194304_158 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %122 = torch.aten.view %120, %121 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   %int-1_159 = torch.constant.int -1
  //   %int32_160 = torch.constant.int 32
  //   %int2_161 = torch.constant.int 2
  //   %int16_162 = torch.constant.int 16
  //   %int32_163 = torch.constant.int 32
  //   %int128_164 = torch.constant.int 128
  //   %123 = torch.prim.ListConstruct %int-1_159, %int32_160, %int2_161, %int16_162, %int32_163, %int128_164 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %124 = torch.aten.view %122, %123 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %none_165 = torch.constant.none
  //   %125 = torch.aten.clone %14, %none_165 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %none_166 = torch.constant.none
  //   %126 = torch.aten.clone %15, %none_166 : !torch.vtensor<[1],si64>, !torch.none -> !torch.vtensor<[1],si64>
  //   %int0_167 = torch.constant.int 0
  //   %127 = torch.aten.unsqueeze %113, %int0_167 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
  //   %int0_168 = torch.constant.int 0
  //   %int3_169 = torch.constant.int 3
  //   %128 = torch.aten.select.int %arg2, %int0_168, %int3_169 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,32,128],f16>
  //   %int0_170 = torch.constant.int 0
  //   %int0_171 = torch.constant.int 0
  //   %129 = torch.aten.select.int %128, %int0_170, %int0_171 : !torch.vtensor<[1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[32,128],f16>
  //   %130 = torch.prim.ListConstruct %112, %125, %126, %127 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
  //   %false_172 = torch.constant.bool false
  //   %131 = torch.aten.index_put %124, %130, %129, %false_172 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[32,128],f16>, !torch.bool -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int256_173 = torch.constant.int 256
  //   %int4194304_174 = torch.constant.int 4194304
  //   %132 = torch.prim.ListConstruct %int256_173, %int4194304_174 : (!torch.int, !torch.int) -> !torch.list<int>
  //   %133 = torch.aten.view %131, %132 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[256,4194304],f16>
  //   torch.overwrite.tensor.contents %133 overwrites %arg6 : !torch.vtensor<[256,4194304],f16>, !torch.tensor<[256,4194304],f16>
  //   %int0_175 = torch.constant.int 0
  //   %int0_176 = torch.constant.int 0
  //   %int9223372036854775807_177 = torch.constant.int 9223372036854775807
  //   %int1_178 = torch.constant.int 1
  //   %134 = torch.aten.slice.Tensor %18, %int0_175, %int0_176, %int9223372036854775807_177, %int1_178 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_179 = torch.constant.int 1
  //   %int0_180 = torch.constant.int 0
  //   %int64 = torch.constant.int 64
  //   %int1_181 = torch.constant.int 1
  //   %135 = torch.aten.slice.Tensor %134, %int1_179, %int0_180, %int64, %int1_181 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,64,32,128],f16>
  //   %int0_182 = torch.constant.int 0
  //   %int0_183 = torch.constant.int 0
  //   %int9223372036854775807_184 = torch.constant.int 9223372036854775807
  //   %int1_185 = torch.constant.int 1
  //   %136 = torch.aten.slice.Tensor %20, %int0_182, %int0_183, %int9223372036854775807_184, %int1_185 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_186 = torch.constant.int 1
  //   %int0_187 = torch.constant.int 0
  //   %int64_188 = torch.constant.int 64
  //   %int1_189 = torch.constant.int 1
  //   %137 = torch.aten.slice.Tensor %136, %int1_186, %int0_187, %int64_188, %int1_189 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,64,32,128],f16>
  //   %int64_190 = torch.constant.int 64
  //   %138 = torch.aten.mul.Scalar %arg5, %int64_190 : !torch.vtensor<[4,4],si64>, !torch.int -> !torch.vtensor<[4,4],si64>
  //   %int0_191 = torch.constant.int 0
  //   %int1_192 = torch.constant.int 1
  //   %139 = torch.aten.add.Scalar %138, %int0_191, %int1_192 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,4],si64>
  //   %int16_193 = torch.constant.int 16
  //   %140 = torch.prim.ListConstruct %int16_193 : (!torch.int) -> !torch.list<int>
  //   %141 = torch.aten.view %139, %140 : !torch.vtensor<[4,4],si64>, !torch.list<int> -> !torch.vtensor<[16],si64>
  //   %int-1_194 = torch.constant.int -1
  //   %int32_195 = torch.constant.int 32
  //   %int2_196 = torch.constant.int 2
  //   %int16_197 = torch.constant.int 16
  //   %int32_198 = torch.constant.int 32
  //   %int128_199 = torch.constant.int 128
  //   %142 = torch.prim.ListConstruct %int-1_194, %int32_195, %int2_196, %int16_197, %int32_198, %int128_199 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %143 = torch.aten.view %133, %142 : !torch.vtensor<[256,4194304],f16>, !torch.list<int> -> !torch.vtensor<[256,32,2,16,32,128],f16>
  //   %int16384 = torch.constant.int 16384
  //   %int16_200 = torch.constant.int 16
  //   %int32_201 = torch.constant.int 32
  //   %int128_202 = torch.constant.int 128
  //   %144 = torch.prim.ListConstruct %int16384, %int16_200, %int32_201, %int128_202 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %145 = torch.aten.view %143, %144 : !torch.vtensor<[256,32,2,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[16384,16,32,128],f16>
  //   %int0_203 = torch.constant.int 0
  //   %146 = torch.aten.index_select %145, %int0_203, %141 : !torch.vtensor<[16384,16,32,128],f16>, !torch.int, !torch.vtensor<[16],si64> -> !torch.vtensor<[16,16,32,128],f16>
  //   %int4_204 = torch.constant.int 4
  //   %int4_205 = torch.constant.int 4
  //   %int16_206 = torch.constant.int 16
  //   %int32_207 = torch.constant.int 32
  //   %int128_208 = torch.constant.int 128
  //   %147 = torch.prim.ListConstruct %int4_204, %int4_205, %int16_206, %int32_207, %int128_208 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %148 = torch.aten.view %146, %147 : !torch.vtensor<[16,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,4,16,32,128],f16>
  //   %int4_209 = torch.constant.int 4
  //   %int64_210 = torch.constant.int 64
  //   %int32_211 = torch.constant.int 32
  //   %int128_212 = torch.constant.int 128
  //   %149 = torch.prim.ListConstruct %int4_209, %int64_210, %int32_211, %int128_212 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %150 = torch.aten.view %148, %149 : !torch.vtensor<[4,4,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,64,32,128],f16>
  //   %false_213 = torch.constant.bool false
  //   %151 = torch.aten.copy %135, %150, %false_213 : !torch.vtensor<[4,64,32,128],f16>, !torch.vtensor<[4,64,32,128],f16>, !torch.bool -> !torch.vtensor<[4,64,32,128],f16>
  //   %int0_214 = torch.constant.int 0
  //   %int0_215 = torch.constant.int 0
  //   %int9223372036854775807_216 = torch.constant.int 9223372036854775807
  //   %int1_217 = torch.constant.int 1
  //   %152 = torch.aten.slice.Tensor %18, %int0_214, %int0_215, %int9223372036854775807_216, %int1_217 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_218 = torch.constant.int 1
  //   %int0_219 = torch.constant.int 0
  //   %int64_220 = torch.constant.int 64
  //   %int1_221 = torch.constant.int 1
  //   %153 = torch.aten.slice_scatter %152, %151, %int1_218, %int0_219, %int64_220, %int1_221 : !torch.vtensor<[4,4096,32,128],f16>, !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int0_222 = torch.constant.int 0
  //   %int0_223 = torch.constant.int 0
  //   %int9223372036854775807_224 = torch.constant.int 9223372036854775807
  //   %int1_225 = torch.constant.int 1
  //   %154 = torch.aten.slice_scatter %18, %153, %int0_222, %int0_223, %int9223372036854775807_224, %int1_225 : !torch.vtensor<[4,4096,32,128],f16>, !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_226 = torch.constant.int 1
  //   %int1_227 = torch.constant.int 1
  //   %155 = torch.aten.add.Scalar %139, %int1_226, %int1_227 : !torch.vtensor<[4,4],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,4],si64>
  //   %int16_228 = torch.constant.int 16
  //   %156 = torch.prim.ListConstruct %int16_228 : (!torch.int) -> !torch.list<int>
  //   %157 = torch.aten.view %155, %156 : !torch.vtensor<[4,4],si64>, !torch.list<int> -> !torch.vtensor<[16],si64>
  //   %int0_229 = torch.constant.int 0
  //   %158 = torch.aten.index_select %145, %int0_229, %157 : !torch.vtensor<[16384,16,32,128],f16>, !torch.int, !torch.vtensor<[16],si64> -> !torch.vtensor<[16,16,32,128],f16>
  //   %int4_230 = torch.constant.int 4
  //   %int4_231 = torch.constant.int 4
  //   %int16_232 = torch.constant.int 16
  //   %int32_233 = torch.constant.int 32
  //   %int128_234 = torch.constant.int 128
  //   %159 = torch.prim.ListConstruct %int4_230, %int4_231, %int16_232, %int32_233, %int128_234 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %160 = torch.aten.view %158, %159 : !torch.vtensor<[16,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,4,16,32,128],f16>
  //   %int4_235 = torch.constant.int 4
  //   %int64_236 = torch.constant.int 64
  //   %int32_237 = torch.constant.int 32
  //   %int128_238 = torch.constant.int 128
  //   %161 = torch.prim.ListConstruct %int4_235, %int64_236, %int32_237, %int128_238 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %162 = torch.aten.view %160, %161 : !torch.vtensor<[4,4,16,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,64,32,128],f16>
  //   %false_239 = torch.constant.bool false
  //   %163 = torch.aten.copy %137, %162, %false_239 : !torch.vtensor<[4,64,32,128],f16>, !torch.vtensor<[4,64,32,128],f16>, !torch.bool -> !torch.vtensor<[4,64,32,128],f16>
  //   %int0_240 = torch.constant.int 0
  //   %int0_241 = torch.constant.int 0
  //   %int9223372036854775807_242 = torch.constant.int 9223372036854775807
  //   %int1_243 = torch.constant.int 1
  //   %164 = torch.aten.slice.Tensor %20, %int0_240, %int0_241, %int9223372036854775807_242, %int1_243 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_244 = torch.constant.int 1
  //   %int0_245 = torch.constant.int 0
  //   %int64_246 = torch.constant.int 64
  //   %int1_247 = torch.constant.int 1
  //   %165 = torch.aten.slice_scatter %164, %163, %int1_244, %int0_245, %int64_246, %int1_247 : !torch.vtensor<[4,4096,32,128],f16>, !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int0_248 = torch.constant.int 0
  //   %int0_249 = torch.constant.int 0
  //   %int9223372036854775807_250 = torch.constant.int 9223372036854775807
  //   %int1_251 = torch.constant.int 1
  //   %166 = torch.aten.slice_scatter %20, %165, %int0_248, %int0_249, %int9223372036854775807_250, %int1_251 : !torch.vtensor<[4,4096,32,128],f16>, !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_252 = torch.constant.int 1
  //   %int2_253 = torch.constant.int 2
  //   %167 = torch.aten.transpose.int %arg0, %int1_252, %int2_253 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
  //   %int0_254 = torch.constant.int 0
  //   %int0_255 = torch.constant.int 0
  //   %int9223372036854775807_256 = torch.constant.int 9223372036854775807
  //   %int1_257 = torch.constant.int 1
  //   %168 = torch.aten.slice.Tensor %154, %int0_254, %int0_255, %int9223372036854775807_256, %int1_257 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_258 = torch.constant.int 1
  //   %int0_259 = torch.constant.int 0
  //   %int64_260 = torch.constant.int 64
  //   %int1_261 = torch.constant.int 1
  //   %169 = torch.aten.slice.Tensor %168, %int1_258, %int0_259, %int64_260, %int1_261 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,64,32,128],f16>
  //   %int1_262 = torch.constant.int 1
  //   %int2_263 = torch.constant.int 2
  //   %170 = torch.aten.transpose.int %169, %int1_262, %int2_263 : !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,64,128],f16>
  //   %int0_264 = torch.constant.int 0
  //   %int0_265 = torch.constant.int 0
  //   %int9223372036854775807_266 = torch.constant.int 9223372036854775807
  //   %int1_267 = torch.constant.int 1
  //   %171 = torch.aten.slice.Tensor %166, %int0_264, %int0_265, %int9223372036854775807_266, %int1_267 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,4096,32,128],f16>
  //   %int1_268 = torch.constant.int 1
  //   %int0_269 = torch.constant.int 0
  //   %int64_270 = torch.constant.int 64
  //   %int1_271 = torch.constant.int 1
  //   %172 = torch.aten.slice.Tensor %171, %int1_268, %int0_269, %int64_270, %int1_271 : !torch.vtensor<[4,4096,32,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,64,32,128],f16>
  //   %int1_272 = torch.constant.int 1
  //   %int2_273 = torch.constant.int 2
  //   %173 = torch.aten.transpose.int %172, %int1_272, %int2_273 : !torch.vtensor<[4,64,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,64,128],f16>
  //   %float0.000000e00 = torch.constant.float 0.000000e+00
  //   %true = torch.constant.bool true
  //   %none_274 = torch.constant.none
  //   %none_275 = torch.constant.none
  //   %174:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%167, %170, %173, %float0.000000e00, %true, %none_274, %none_275) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,64,128],f16>, !torch.vtensor<[4,32,64,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
  //   %int1_276 = torch.constant.int 1
  //   %int2_277 = torch.constant.int 2
  //   %175 = torch.aten.transpose.int %174#0, %int1_276, %int2_277 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
  //   %int4_278 = torch.constant.int 4
  //   %int1_279 = torch.constant.int 1
  //   %int-1_280 = torch.constant.int -1
  //   %176 = torch.prim.ListConstruct %int4_278, %int1_279, %int-1_280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  //   %177 = torch.aten.view %175, %176 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
  //   return %177 : !torch.vtensor<[4,1,4096],f16>
  // }
}

{-#
  dialect_resources: {
    builtin: {
    }
  }
#-}
