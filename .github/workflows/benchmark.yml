# Copyright 2023 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

name: Benchmark CI

on:
  workflow_dispatch:
  pull_request:
    types:
      - synchronize
      - opened
      - reopened
      - labeled
      - unlabeled
      - edited
  push:
    branches:
      - main

concurrency:
  # A PR number if a pull request and otherwise the commit hash. This cancels
  # queued and in-progress runs for the same PR (presubmit) or commit
  # (postsubmit).
  group: ${{ github.workflow }}-${{ github.event.number || github.sha }}
  cancel-in-progress: true

env:
  # This needs to be in env instead of the outputs of setup because it contains
  # the run attempt and we want that to be the current attempt, not whatever
  # attempt the setup step last ran in.
  GCS_DIR: gs://iree-github-actions-${{ github.event_name == 'pull_request' && 'presubmit' || 'postsubmit' }}-artifacts/${{ github.run_id }}/${{ github.run_attempt }}
  HEAD_SHA: ${{ github.event.pull_request.head.sha || github.sha }}

jobs:
  setup:
    runs-on: ubuntu-20.04
    env:
      # The commit being checked out is the merge commit for the PR. Its first
      # parent will be the tip of main.
      BASE_REF: HEAD^
      PR_TITLE: ${{ github.event.pull_request.title }}
      PR_BODY: ${{ github.event.pull_request.body }}
    outputs:
      should-run: ${{ steps.configure.outputs.should-run }}
      ci-stage: ${{ steps.configure.outputs.ci-stage }}
      runner-env: ${{ steps.configure.outputs.runner-env }}
      runner-group: ${{ steps.configure.outputs.runner-group }}
      write-caches: ${{ steps.configure.outputs.write-caches }}
      benchmark-presets: ${{ steps.configure.outputs.benchmark-presets }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@e2f20e631ae6d7dd3b768f56a5d2af784dd54791 # v2.5.0
        with:
          # We need the parent commit to do a diff
          fetch-depth: 2
      - name: "Configuring CI options"
        id: configure
        run: |
          # Just informative logging. There should only be two commits in the
          # history here, but limiting the depth helps when copying from a local
          # repo instead of using checkout, e.g. with
          # https://github.com/nektos/act where there will be more.
          git log --oneline --graph --max-count=3
          ./build_tools/github_actions/configure_ci.py

  wait_build_all:
    concurrency:
      group: ${{ github.workflow }}-${{ github.sha }}
      cancel-in-progress: true
    needs: setup
    if: needs.setup.outputs.should-run == 'true'
    runs-on: ubuntu-20.04
    outputs:
      wait-conclusion: ${{ steps.wait.outputs.wait-conclusion }}
      build-dir: ${{ steps.wait.outputs.build-dir }}
      build-dir-archive: ${{ steps.wait.outputs.build-dir-archive }}
      build-dir-gcs-artifact: ${{ steps.wait.outputs.build-dir-gcs-artifact }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@e2f20e631ae6d7dd3b768f56a5d2af784dd54791 # v2.5.0
      - name: "Waiting for build_all"
        id: wait
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ./build_tools/github_actions/wait_ci.py \
            --output_variables="${GITHUB_OUTPUT}" \
            build_all

  wait_build_e2e_test_artifacts:
    needs: setup
    if: needs.setup.outputs.should-run == 'true'
    runs-on: ubuntu-20.04
    outputs:
      wait-conclusion: ${{ steps.wait.outputs.wait-conclusion }}
      # Hardcoded artifacts dir name to save unfriendly variables passing
      # between workflows.
      e2e-test-artifacts-dir: e2e_test_artifacts
      e2e-test-artifacts-gcs-artifact-dir: ${{ steps.wait.outputs.e2e-test-artifacts-gcs-artifact-dir }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@e2f20e631ae6d7dd3b768f56a5d2af784dd54791 # v2.5.0
      - name: "Waiting for build_all"
        id: wait
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ./build_tools/github_actions/wait_ci.py \
            --output_variables="${GITHUB_OUTPUT}" \
            build_e2e_test_artifacts

  build_benchmark_tools:
    needs: [setup, wait_build_all]
    if: needs.setup.outputs.should-run == 'true' && needs.wait_build_all.outputs.wait-conclusion == 'success'
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ needs.setup.outputs.runner-group }}
      - environment=${{ needs.setup.outputs.runner-env }}
      - cpu
      - os-family=Linux
    outputs:
      # We can't collect all outputs from the matrix jobs due to Github's
      # limitation (https://github.com/orgs/community/discussions/17245).
      # Therefore, the output is the GCS directory that stores all benchmark
      # tools archives. The following jobs need to construct the archive names
      # by themselves and combine with path of GCS directory here to fetch the
      # archives.
      benchmark-tools-gcs-artifact-dir: ${{ steps.upload.outputs.benchmark-tools-gcs-artifact-dir }}
    strategy:
      matrix:
        target:
          - platform: "linux"
            arch: "x86_64"
            docker_image: "gcr.io/iree-oss/base@sha256:f26a5aa5f8d3705c6b80c71d04fafb360861f1907bdd1b1f5f19480b6192664e"
            # Builds tools on the host and assumes the builder is Linux x86_64.
            build_script: "./build_tools/cmake/build_runtime.sh"
          - platform: "linux"
            arch: "riscv_64"
            docker_image: "gcr.io/iree-oss/riscv@sha256:a7e7b03236324f92e7c96925add02cf0b37eca5b68af54f64049fc689f770915"
            build_script: "./build_tools/cmake/build_riscv.sh"
    env:
      PLATFORM: ${{ matrix.target.platform }}
      ARCH: ${{ matrix.target.arch }}
      DOCKER_IMAGE: ${{ matrix.target.docker_image }}
      BUILD_SCRIPT: ${{ matrix.target.build_script }}
      BUILD_TOOLS_DIR: ${{ matrix.target.platform }}-${{ matrix.target.arch }}-benchmark-tools-dir
      BUILD_DIR: ${{ needs.wait_build_all.outputs.build-dir }}
      BUILD_DIR_ARCHIVE: ${{ needs.wait_build_all.outputs.build-dir-archive }}
      BUILD_DIR_GCS_ARTIFACT: ${{ needs.wait_build_all.outputs.build-dir-gcs-artifact }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@e2f20e631ae6d7dd3b768f56a5d2af784dd54791 # v2.5.0
      - name: "Checking out runtime submodules"
        run: ./build_tools/scripts/git/update_runtime_submodules.sh
      - name: "Downloading build dir archive"
        run: gcloud storage cp "${BUILD_DIR_GCS_ARTIFACT}" "${BUILD_DIR_ARCHIVE}"
      - name: "Extracting host binaries"
        run: tar -xf "${BUILD_DIR_ARCHIVE}" "${BUILD_DIR}/install"
      - name: "Compiling the benchmark tools"
        id: build
        run: |
          ./build_tools/github_actions/docker_run.sh \
            --env "IREE_TARGET_PLATFORM=${PLATFORM}" \
            --env "IREE_TARGET_ARCH=${ARCH}" \
            --env "BUILD_PRESET=benchmark" \
            --env "IREE_HOST_BIN_DIR=${BUILD_DIR}/install/bin" \
            "${DOCKER_IMAGE}" "${BUILD_SCRIPT}" "${BUILD_TOOLS_DIR}/build"
      - name: "Compiling the benchmark tools with tracing"
        id: build-with-tracing
        run: |
          ./build_tools/github_actions/docker_run.sh \
            --env "IREE_TARGET_PLATFORM=${PLATFORM}" \
            --env "IREE_TARGET_ARCH=${ARCH}" \
            --env "BUILD_PRESET=benchmark-with-tracing" \
            --env "IREE_HOST_BIN_DIR=${BUILD_DIR}/install/bin" \
            "${DOCKER_IMAGE}" "${BUILD_SCRIPT}" "${BUILD_TOOLS_DIR}/build-traced"
      - name: "Creating the benchmark tools archive"
        id: archive
        env:
          BENCHMARK_TOOLS_ARCHIVE: ${{ matrix.target.platform }}-${{ matrix.target.arch }}-benchmark-tools.tar
        run: |
          tar -cf "${BENCHMARK_TOOLS_ARCHIVE}" \
            "${BUILD_TOOLS_DIR}"/*/tools/iree-benchmark-module \
            "${BUILD_TOOLS_DIR}"/*/tools/build_config.txt
          echo "benchmark-tools-archive=${BENCHMARK_TOOLS_ARCHIVE}" >> "${GITHUB_OUTPUT}"
      - name: "Uploading the benchmark tools archive"
        id: upload
        env:
          BENCHMARK_TOOLS_ARCHIVE: ${{ steps.archive.outputs.benchmark-tools-archive }}
          BENCHMARK_TOOLS_GCS_ARTIFACT_DIR: ${{ env.GCS_DIR }}/benchmark-tools
        run: |
          gcloud storage cp "${BENCHMARK_TOOLS_ARCHIVE}" "${BENCHMARK_TOOLS_GCS_ARTIFACT_DIR}/"
          echo "benchmark-tools-gcs-artifact-dir=${BENCHMARK_TOOLS_GCS_ARTIFACT_DIR}" >> "${GITHUB_OUTPUT}"

  compilation_benchmarks:
    needs: [setup, wait_build_e2e_test_artifacts]
    if: |
      needs.setup.outputs.should-run == 'true' &&
      needs.wait_build_e2e_test_artifacts.outputs.wait-conclusion == 'success' &&
      needs.setup.outputs.benchmark-presets != ''
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ needs.setup.outputs.runner-group }}
      - environment=${{ needs.setup.outputs.runner-env }}
      - cpu
      - os-family=Linux
    env:
      E2E_TEST_ARTIFACTS_DIR: ${{ needs.wait_build_e2e_test_artifacts.outputs.e2e-test-artifacts-dir }}
      E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR: ${{ needs.wait_build_e2e_test_artifacts.outputs.e2e-test-artifacts-gcs-artifact-dir }}
    outputs:
      compile-stats-results: ${{ steps.collect.outputs.compile-stats-results }}
      compile-stats-results-gcs-artifact: ${{ steps.upload.outputs.compile-stats-results-gcs-artifact }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
      - name: "Exporting configs"
        id: "export"
        env:
          COMPILATION_CONFIG: "compilation-config.json"
        run: |
          ./build_tools/benchmarks/export_benchmark_config.py \
            compilation \
            --output="${COMPILATION_CONFIG}"
          echo "compilation-config=${COMPILATION_CONFIG}" >> "${GITHUB_OUTPUT}"
      - name: "Downloading assets"
        id: "download-assets"
        env:
          E2E_TEST_ARTIFACTS_BUILD_LOG: ${{ env.E2E_TEST_ARTIFACTS_DIR }}/ninja_log
          E2E_TEST_ARTIFACTS_BUILD_LOG_GCS_ARTIFACT: ${{ env.E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR }}/ninja_log
          COMPILATION_CONFIG: ${{ steps.export.outputs.compilation-config }}
        run: |
          mkdir -p "${E2E_TEST_ARTIFACTS_DIR}"
          gcloud storage cp \
            "${E2E_TEST_ARTIFACTS_BUILD_LOG_GCS_ARTIFACT}" \
            "${E2E_TEST_ARTIFACTS_BUILD_LOG}"
          jq -r \
            --arg GCS_ARTIFACT_DIR "${E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR}" \
            '.module_dir_paths | map("\($GCS_ARTIFACT_DIR)/\(.)") | join("\n")' \
            "${COMPILATION_CONFIG}" | \
            gcloud storage cp -r --read-paths-from-stdin \
              "${E2E_TEST_ARTIFACTS_DIR}"
          echo "e2e-test-artifacts-build-log=${E2E_TEST_ARTIFACTS_BUILD_LOG}" >> "${GITHUB_OUTPUT}"
      - name: "Collecting compilation statistics"
        id: collect
        env:
          E2E_TEST_ARTIFACTS_BUILD_LOG: ${{ steps.download-assets.outputs.e2e-test-artifacts-build-log }}
          COMPILATION_CONFIG: ${{ steps.export.outputs.compilation-config }}
          GENERATION_CONFIG: generation-config.json
          COMPILE_STATS_RESULTS: benchmark-results/compile-stats-results.json
        run: |
          jq '.generation_configs' "${COMPILATION_CONFIG}" > "${GENERATION_CONFIG}"
          mkdir -p benchmark-results
          ./build_tools/benchmarks/collect_compilation_statistics.py alpha \
            --e2e_test_artifacts_dir="${E2E_TEST_ARTIFACTS_DIR}" \
            --build_log="${E2E_TEST_ARTIFACTS_BUILD_LOG}" \
            --generation_config="${GENERATION_CONFIG}" \
            --output="${COMPILE_STATS_RESULTS}"
          echo "compile-stats-results=${COMPILE_STATS_RESULTS}" >> "${GITHUB_OUTPUT}"
      - name: "Uploading benchmark results"
        id: upload
        env:
          COMPILATION_CONFIG: ${{ steps.export.outputs.compilation-config }}
          COMPILE_STATS_RESULTS: ${{ steps.collect.outputs.compile-stats-results }}
          COMPILE_STATS_RESULTS_GCS_ARTIFACT: ${{ env.GCS_DIR }}/${{ steps.collect.outputs.compile-stats-results }}
        run: |
          # Upload files with two commands since they go into different GCS
          # directories.
          gcloud storage cp "${COMPILATION_CONFIG}" "${GCS_DIR}"
          gcloud storage cp \
            "${COMPILE_STATS_RESULTS}" \
            "${COMPILE_STATS_RESULTS_GCS_ARTIFACT}"
          echo "compile-stats-results-gcs-artifact=${COMPILE_STATS_RESULTS_GCS_ARTIFACT}" >> "${GITHUB_OUTPUT}"

  execution_benchmarks:
    needs: [setup, build_benchmark_tools, wait_build_e2e_test_artifacts]
    if: |
      needs.setup.outputs.should-run == 'true' &&
      needs.wait_build_e2e_test_artifacts.outputs.wait-conclusion == 'success' &&
      needs.setup.outputs.benchmark-presets != ''
    uses: ./.github/workflows/benchmark_execution.yml
    with:
      # env.GCS_DIR is also duplicated in this workflow. See the note there on
      # why this is.
      runner-group: ${{ needs.setup.outputs.runner-group }}
      runner-env: ${{ needs.setup.outputs.runner-env }}
      e2e-test-artifacts-dir: ${{ needs.wait_build_e2e_test_artifacts.outputs.e2e-test-artifacts-dir }}
      e2e-test-artifacts-gcs-artifact-dir: ${{ needs.wait_build_e2e_test_artifacts.outputs.e2e-test-artifacts-gcs-artifact-dir }}
      benchmark-tools-gcs-artifact-dir: ${{ needs.build_benchmark_tools.outputs.benchmark-tools-gcs-artifact-dir }}
      benchmark-presets: ${{ needs.setup.outputs.benchmark-presets }}

  process_benchmark_results:
    needs: [setup, compilation_benchmarks, execution_benchmarks]
    if: needs.setup.outputs.should-run == 'true' && needs.setup.outputs.benchmark-presets != ''
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ needs.setup.outputs.runner-group }}
      - environment=${{ needs.setup.outputs.runner-env }}
      - cpu
      - os-family=Linux
    env:
      COMPILE_STATS_RESULTS: ${{ needs.compilation_benchmarks.outputs.compile-stats-results }}
      COMPILE_STATS_RESULTS_GCS_ARTIFACT: ${{ needs.compilation_benchmarks.outputs.compile-stats-results-gcs-artifact }}
      # Empty if no execution benchmark runs.
      EXECUTION_BENCHMARK_RESULTS_DIR: ${{ needs.execution_benchmarks.outputs.benchmark-results-dir }}
      # Empty if no execution benchmark runs.
      EXECUTION_BENCHMARK_RESULTS_GCS_ARTIFACT_DIR: ${{ needs.execution_benchmarks.outputs.benchmark-results-gcs-artifact-dir }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
        with:
          # We need the full history (and main branch) to generate the report.
          fetch-depth: 0
      - name: Downloading compilation benchmark results
        run: |
          gcloud storage cp \
            "${COMPILE_STATS_RESULTS_GCS_ARTIFACT}" \
            "${COMPILE_STATS_RESULTS}"
      - name: Downloading execution benchmark results
        id: download-execution-results
        # Skip the download if there is no execution benchmark results (e.g. no
        # benchmark matches the preset/filter). In such case, no benchmark job
        # is run in benchmark_execution.yml and the output variables are empty.
        if: env.EXECUTION_BENCHMARK_RESULTS_GCS_ARTIFACT_DIR != ''
        run: |
          gcloud storage cp -r \
            "${EXECUTION_BENCHMARK_RESULTS_GCS_ARTIFACT_DIR}/benchmark-results-*.json" \
            "${EXECUTION_BENCHMARK_RESULTS_DIR}"
          echo "execution-benchmark-results-pattern=${EXECUTION_BENCHMARK_RESULTS_DIR}/benchmark-results-*.json" >> "${GITHUB_OUTPUT}"
      - name: Generating comment
        if: needs.setup.outputs.ci-stage == 'presubmit'
        id: generate-comment
        env:
          # Wildcard pattern to match all execution benchmark results. Empty if
          # execution_benchmarks is skipped, which results in no match.
          EXECUTION_BENCHMARK_RESULTS_PATTERN: ${{ steps.download-execution-results.outputs.execution-benchmark-results-pattern }}
          IREE_BUILD_URL: https://github.com/openxla/iree/actions/runs/${{ github.run_id }}/attempts/${{ github.run_attempt }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BENCHMARK_COMMENT_ARTIFACT: benchmark-comment.json
        run: |
          build_tools/github_actions/docker_run.sh \
            gcr.io/iree-oss/benchmark-report@sha256:7498c6f32f63f13faf085463cc38656d4297519c824e63e1c99c8c258147f6ff \
            ./build_tools/benchmarks/generate_benchmark_comment.py \
              --verbose \
              --pr_number="${PR_NUMBER}" \
              --pr_committish="${GITHUB_SHA}" \
              --pr_base_branch="origin/${GITHUB_BASE_REF}" \
              --comment_type="benchmark-summary" \
              --build_url="${IREE_BUILD_URL}" \
              --benchmark_files="${EXECUTION_BENCHMARK_RESULTS_PATTERN}" \
              --compile_stats_files="${COMPILE_STATS_RESULTS}" \
              --output="${BENCHMARK_COMMENT_ARTIFACT}"
          echo "benchmark-comment-artifact=${BENCHMARK_COMMENT_ARTIFACT}" >> "${GITHUB_OUTPUT}"
      - name: Uploading comment artifact
        # Due to security reasons, instead of posting the comment to PR, we only
        # upload the comment data in presubmit workflow and trigger the posting
        # workflow on the main branch. See post_benchmark_comment.yaml
        if: needs.setup.outputs.ci-stage == 'presubmit'
        env:
          BENCHMARK_COMMENT_ARTIFACT: ${{ steps.generate-comment.outputs.benchmark-comment-artifact }}
          BENCHMARK_COMMENT_GCS_ARTIFACT: ${{ env.GCS_DIR }}/${{ steps.generate-comment.outputs.benchmark-comment-artifact }}
        run: |
          gcloud storage cp \
            "${BENCHMARK_COMMENT_ARTIFACT}" \
            "${BENCHMARK_COMMENT_GCS_ARTIFACT}"
          echo "::notice title=JOB_OUTPUT_VARIABLE::benchmark-comment-gcs-artifact=${BENCHMARK_COMMENT_GCS_ARTIFACT}"
      - name: Uploading results to dashboard
        if: needs.setup.outputs.ci-stage == 'postsubmit'
        env:
          EXECUTION_BENCHMARK_RESULTS_PATTERN: ${{ steps.download-execution-results.outputs.execution-benchmark-results-pattern }}
          IREE_DASHBOARD_API_TOKEN: ${{ secrets.IREE_DASHBOARD_API_TOKEN }}
        run: |
          build_tools/github_actions/docker_run.sh \
            --env "IREE_DASHBOARD_API_TOKEN=${IREE_DASHBOARD_API_TOKEN}" \
            gcr.io/iree-oss/benchmark-report@sha256:7498c6f32f63f13faf085463cc38656d4297519c824e63e1c99c8c258147f6ff \
            ./build_tools/benchmarks/upload_benchmarks_to_dashboard.py \
              --verbose \
              --benchmark_files="${EXECUTION_BENCHMARK_RESULTS_PATTERN}" \
              --compile_stats_files="${COMPILE_STATS_RESULTS}"
