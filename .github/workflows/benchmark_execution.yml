# Copyright 2022 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# Workflow for execution-benchmark-related jobs. It is designed to be called
# from the main workflow ci.yml. The concurrency of this workflow is controlled
# by the caller's job.

name: Benchmark execution

on:
  workflow_call:
    inputs:
      runner-group:
        required: true
        type: string
      runner-env:
        required: true
        type: string
      ci-stage:
        required: true
        type: string
      e2e-test-artifacts-dir:
        required: true
        type: string
      e2e-test-artifacts-gcs-artifact-dir:
        required: true
        type: string
      benchmark-tools-gcs-artifact-dir:
        required: true
        type: string
      benchmark-presets:
        required: false
        type: string

env:
  # This duplicates the variable from ci.yml. The variable needs to be in env
  # instead of the outputs of setup because it contains the run attempt and we
  # want that to be the current attempt, not whatever attempt the setup step
  # last ran in. It therefore can't be passed in via inputs because the env
  # context isn't available there.
  GCS_DIR: gs://iree-github-actions-${{ github.event_name == 'pull_request' && 'presubmit' || 'postsubmit' }}-artifacts/${{ github.run_id }}/${{ github.run_attempt }}

concurrency:
  # A PR number if a pull request and otherwise the commit hash. This cancels
  # queued and in-progress runs for the same PR (presubmit) or commit
  # (postsubmit).
  group: launch-benchmarks-${{ github.event.number || github.sha }}

jobs:
  # dummy_first:
  #   runs-on: ubuntu-20.04
  #   steps:
  #     - name: "Dummy"
  #       run: sleep 1

  # dummy:
  #   needs: [dummy_first]
  #   runs-on: ubuntu-20.04
  #   steps:
  #     - name: "Dummy"
  #       run: sleep 1000

  get_benchmark_signal:
    runs-on: ubuntu-20.04
    outputs:
      benchmark-presets: ${{ steps.publish.outputs.benchmark-presets }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
      - name: "Detecting benchmark presets"
        id: detect
        if: github.event_name == 'pull_request'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PULL_NUMBER: ${{ github.event.number }}
          PR_JSON: pr.json
        run: |
          gh api "/repos/${GITHUB_REPOSITORY}/pulls/${PULL_NUMBER}" > "${PR_JSON}"
          build_tools/github_actions/detect_benchmark_signal.sh "${PR_JSON}"
      - name: "Publishing benchmark presets"
        id: publish
        env:
          BENCHMARK_PRESETS: ${{ steps.detect.outputs.benchmark-presets || inputs.benchmark-presets }}
        run: |
          echo "::notice title=Running Benchmark Presets::${BENCHMARK_PRESETS}"
          echo "benchmark-presets=${BENCHMARK_PRESETS}" >> "${GITHUB_OUTPUT}"

  export_benchmark_config:
    needs: [get_benchmark_signal]
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ inputs.runner-group }}
      - environment=${{ inputs.runner-env }}
      - cpu
      - os-family=Linux
    outputs:
      benchmark-matrix: ${{ steps.export.outputs.benchmark-matrix }}
      benchmark-config: ${{ steps.export.outputs.benchmark-config }}
      benchmark-config-gcs-artifact: ${{ steps.upload.outputs.benchmark-config-gcs-artifact }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
      - name: "Checking out runtime submodules"
        run: ./build_tools/scripts/git/update_runtime_submodules.sh
      - name: "Exporting benchmark run config"
        id: export
        env:
          BENCHMARK_CONFIG: "benchmark-config.json"
          BENCHMARK_PRESETS: ${{ needs.get_benchmark_signal.outputs.benchmark-presets }}
        run: |
          ./build_tools/benchmarks/export_benchmark_config.py \
            execution \
            --benchmark_presets="${BENCHMARK_PRESETS}" \
            --output="${BENCHMARK_CONFIG}"
          echo "benchmark-config=${BENCHMARK_CONFIG}" >> "${GITHUB_OUTPUT}"
          echo benchmark-matrix=$(jq \
              'to_entries | map({"device_name": .key, "host_environment": .value.host_environment})' \
              "${BENCHMARK_CONFIG}") \
            >> "${GITHUB_OUTPUT}"
      - name: "Uploading benchmark config"
        id: upload
        if: steps.export.outputs.benchmark-matrix != '[]'
        env:
          BENCHMARK_CONFIG: ${{ steps.export.outputs.benchmark-config }}
          BENCHMARK_CONFIG_GCS_ARTIFACT: ${{ env.GCS_DIR }}/${{ steps.export.outputs.benchmark-config }}
        run: |
          gcloud storage cp \
            "${BENCHMARK_CONFIG}" \
            "${BENCHMARK_CONFIG_GCS_ARTIFACT}"
          echo "benchmark-config-gcs-artifact=${BENCHMARK_CONFIG_GCS_ARTIFACT}" >> "${GITHUB_OUTPUT}"

  execution_benchmarks:
    needs: [export_benchmark_config]
    if: needs.export_benchmark_config.outputs.benchmark-matrix != '[]'
    strategy:
      # Matrix is dynamically generated by the job export_benchmark_config. So
      # we only runs the benchmarks specified in inputs.benchmark-presets.
      # All tasks in matrix are seen as a single job in Github CI and the job
      # can output a single set of values.
      matrix:
        benchmark: ${{ fromJSON(needs.export_benchmark_config.outputs.benchmark-matrix) }}
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ inputs.runner-group }}
      - environment=${{ inputs.runner-env }}
      - machine-type=${{ matrix.benchmark.device_name }}
    env:
      DEVICE_NAME: ${{ matrix.benchmark.device_name }}
      PLATFORM_ARCH: ${{ matrix.benchmark.host_environment.platform }}-${{ matrix.benchmark.host_environment.architecture }}
      E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR: ${{ inputs.e2e-test-artifacts-gcs-artifact-dir }}
      E2E_TEST_ARTIFACTS_DIR: ${{ inputs.e2e-test-artifacts-dir }}
      BENCHMARK_RESULTS_DIR: benchmark-results
    outputs:
      benchmark-results-dir: ${{ env.BENCHMARK_RESULTS_DIR }}
      # Ideally this should be defined in env, so it can be used in the upload
      # step. But Github CI doesn't allow us to access env.GCS in env.
      benchmark-results-gcs-artifact-dir: ${{ env.GCS_DIR }}/${{ env.BENCHMARK_RESULTS_DIR }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
      - name: "Checking out runtime submodules"
        run: ./build_tools/scripts/git/update_runtime_submodules.sh
      - name: "Downloading benchmark tools"
        id: download-tools
        env:
          # See `build_benchmark_tools` step in ci.yml for the name format of
          # benchmark tools artifacts.
          BENCHMARK_TOOLS_ARCHIVE: ${{ env.PLATFORM_ARCH }}-benchmark-tools.tar
          BENCHMARK_TOOLS_GCS_ARTIFACT: ${{ inputs.benchmark-tools-gcs-artifact-dir }}/${{ env.PLATFORM_ARCH }}-benchmark-tools.tar
        run: |
          gcloud storage cp "${BENCHMARK_TOOLS_GCS_ARTIFACT}" "${BENCHMARK_TOOLS_ARCHIVE}"
          echo "benchmark-tools-archive=${BENCHMARK_TOOLS_ARCHIVE}" >> "${GITHUB_OUTPUT}"
      - name: "Downloading benchmark assets"
        id: download-assets
        env:
          BENCHMARK_CONFIG: ${{ needs.export_benchmark_config.outputs.benchmark-config }}
          BENCHMARK_CONFIG_GCS_ARTIFACT: ${{ needs.export_benchmark_config.outputs.benchmark-config-gcs-artifact }}
        run: |
          gcloud storage cp "${BENCHMARK_CONFIG_GCS_ARTIFACT}" "${BENCHMARK_CONFIG}"
          mkdir -p "${E2E_TEST_ARTIFACTS_DIR}"
          jq -r \
            --arg DEVICE_NAME "${DEVICE_NAME}" \
            --arg GCS_ARTIFACT_DIR "${E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR}" \
            '.[$DEVICE_NAME] | .module_dir_paths | map("\($GCS_ARTIFACT_DIR)/\(.)") | join("\n")' \
            "${BENCHMARK_CONFIG}" | \
            gcloud storage cp -r --read-paths-from-stdin "${E2E_TEST_ARTIFACTS_DIR}"
          echo "benchmark-config=${BENCHMARK_CONFIG}" >> "${GITHUB_OUTPUT}"
      - name: "Unpacking benchmark tools"
        id: unpack-tools
        env:
          BENCHMARK_TOOLS_ARCHIVE: ${{ steps.download-tools.outputs.benchmark-tools-archive }}
          # See `build_benchmark_tools` step in ci.yml for the name format of
          # benchmark tools directory.
          BENCHMARK_TOOLS_DIR: ${{ env.PLATFORM_ARCH }}-benchmark-tools-dir
        run: |
          tar -xf ${BENCHMARK_TOOLS_ARCHIVE}
          echo "normal-benchmark-tools-dir=${BENCHMARK_TOOLS_DIR}/build/tools" >> "${GITHUB_OUTPUT}"
          echo "traced-benchmark-tools-dir=${BENCHMARK_TOOLS_DIR}/build-traced/tools" >> "${GITHUB_OUTPUT}"
      - name: "Running benchmarks"
        id: run
        env:
          BENCHMARK_CONFIG: ${{ steps.download-assets.outputs.benchmark-config }}
          IREE_DOCKER_WRAPPER: ./build_tools/github_actions/docker_run.sh
          IREE_NORMAL_BENCHMARK_TOOLS_DIR: ${{ steps.unpack-tools.outputs.normal-benchmark-tools-dir }}
          IREE_TRACED_BENCHMARK_TOOLS_DIR: ${{ steps.unpack-tools.outputs.traced-benchmark-tools-dir }}
          IREE_DEVICE_NAME: ${{ env.DEVICE_NAME }}
          IREE_E2E_TEST_ARTIFACTS_DIR: ${{ env.E2E_TEST_ARTIFACTS_DIR }}
          IREE_RUN_CONFIG: run-config.json
          IREE_BENCHMARK_RESULTS: ${{ env.BENCHMARK_RESULTS_DIR }}/benchmark-results-${{ matrix.benchmark.device_name }}.json
        run: |
          mkdir -p ${BENCHMARK_RESULTS_DIR}
          jq --arg DEVICE_NAME "${IREE_DEVICE_NAME}" \
            '.[$DEVICE_NAME] | .run_configs' \
            "${BENCHMARK_CONFIG}" > "${IREE_RUN_CONFIG}"
          ./build_tools/benchmarks/run_benchmarks.sh
          echo "benchmark-results=${IREE_BENCHMARK_RESULTS}" >> "${GITHUB_OUTPUT}"
      - name: "Uploading benchmark results"
        id: upload
        env:
          BENCHMARK_RESULTS: ${{ steps.run.outputs.benchmark-results }}
        run: |
          gcloud storage cp \
            "${BENCHMARK_RESULTS}" \
            "${GCS_DIR}/${BENCHMARK_RESULTS}"

  compilation_benchmarks:
    needs: [get_benchmark_signal]
    if: needs.get_benchmark_signal.outputs.benchmark-presets != ''
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ inputs.runner-group }}
      - environment=${{ inputs.runner-env }}
      - cpu
      - os-family=Linux
    env:
      E2E_TEST_ARTIFACTS_DIR: ${{ inputs.e2e-test-artifacts-dir }}
      E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR: ${{ inputs.e2e-test-artifacts-gcs-artifact-dir }}
    outputs:
      compile-stats-results: ${{ steps.collect.outputs.compile-stats-results }}
      compile-stats-results-gcs-artifact: ${{ steps.upload.outputs.compile-stats-results-gcs-artifact }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
      - name: "Exporting configs"
        id: "export"
        env:
          COMPILATION_CONFIG: "compilation-config.json"
        run: |
          ./build_tools/benchmarks/export_benchmark_config.py \
            compilation \
            --output="${COMPILATION_CONFIG}"
          echo "compilation-config=${COMPILATION_CONFIG}" >> "${GITHUB_OUTPUT}"
      - name: "Downloading assets"
        id: "download-assets"
        env:
          E2E_TEST_ARTIFACTS_BUILD_LOG: ${{ env.E2E_TEST_ARTIFACTS_DIR }}/ninja_log
          E2E_TEST_ARTIFACTS_BUILD_LOG_GCS_ARTIFACT: ${{ env.E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR }}/ninja_log
          COMPILATION_CONFIG: ${{ steps.export.outputs.compilation-config }}
        run: |
          mkdir -p "${E2E_TEST_ARTIFACTS_DIR}"
          gcloud storage cp \
            "${E2E_TEST_ARTIFACTS_BUILD_LOG_GCS_ARTIFACT}" \
            "${E2E_TEST_ARTIFACTS_BUILD_LOG}"
          jq -r \
            --arg GCS_ARTIFACT_DIR "${E2E_TEST_ARTIFACTS_GCS_ARTIFACT_DIR}" \
            '.module_dir_paths | map("\($GCS_ARTIFACT_DIR)/\(.)") | join("\n")' \
            "${COMPILATION_CONFIG}" | \
            gcloud storage cp -r --read-paths-from-stdin \
              "${E2E_TEST_ARTIFACTS_DIR}"
          echo "e2e-test-artifacts-build-log=${E2E_TEST_ARTIFACTS_BUILD_LOG}" >> "${GITHUB_OUTPUT}"
      - name: "Collecting compilation statistics"
        id: collect
        env:
          E2E_TEST_ARTIFACTS_BUILD_LOG: ${{ steps.download-assets.outputs.e2e-test-artifacts-build-log }}
          COMPILATION_CONFIG: ${{ steps.export.outputs.compilation-config }}
          GENERATION_CONFIG: generation-config.json
          COMPILE_STATS_RESULTS: benchmark-results/compile-stats-results.json
        run: |
          jq '.generation_configs' "${COMPILATION_CONFIG}" > "${GENERATION_CONFIG}"
          mkdir -p benchmark-results
          ./build_tools/benchmarks/collect_compilation_statistics.py alpha \
            --e2e_test_artifacts_dir="${E2E_TEST_ARTIFACTS_DIR}" \
            --build_log="${E2E_TEST_ARTIFACTS_BUILD_LOG}" \
            --generation_config="${GENERATION_CONFIG}" \
            --output="${COMPILE_STATS_RESULTS}"
          echo "compile-stats-results=${COMPILE_STATS_RESULTS}" >> "${GITHUB_OUTPUT}"
      - name: "Uploading benchmark results"
        id: upload
        env:
          COMPILATION_CONFIG: ${{ steps.export.outputs.compilation-config }}
          COMPILE_STATS_RESULTS: ${{ steps.collect.outputs.compile-stats-results }}
          COMPILE_STATS_RESULTS_GCS_ARTIFACT: ${{ env.GCS_DIR }}/${{ steps.collect.outputs.compile-stats-results }}
        run: |
          # Upload files with two commands since they go into different GCS
          # directories.
          gcloud storage cp "${COMPILATION_CONFIG}" "${GCS_DIR}"
          gcloud storage cp \
            "${COMPILE_STATS_RESULTS}" \
            "${COMPILE_STATS_RESULTS_GCS_ARTIFACT}"
          echo "compile-stats-results-gcs-artifact=${COMPILE_STATS_RESULTS_GCS_ARTIFACT}" >> "${GITHUB_OUTPUT}"

  process_benchmark_results:
    needs: [compilation_benchmarks, execution_benchmarks]
    runs-on:
      - self-hosted  # must come first
      - runner-group=${{ inputs.runner-group }}
      - environment=${{ inputs.runner-env }}
      - cpu
      - os-family=Linux
    env:
      COMPILE_STATS_RESULTS: ${{ needs.compilation_benchmarks.outputs.compile-stats-results }}
      COMPILE_STATS_RESULTS_GCS_ARTIFACT: ${{ needs.compilation_benchmarks.outputs.compile-stats-results-gcs-artifact }}
      # Empty if no execution benchmark runs.
      EXECUTION_BENCHMARK_RESULTS_DIR: ${{ needs.execution_benchmarks.outputs.benchmark-results-dir }}
      # Empty if no execution benchmark runs.
      EXECUTION_BENCHMARK_RESULTS_GCS_ARTIFACT_DIR: ${{ needs.execution_benchmarks.outputs.benchmark-results-gcs-artifact-dir }}
    steps:
      - name: "Checking out repository"
        uses: actions/checkout@7884fcad6b5d53d10323aee724dc68d8b9096a2e # v2
        with:
          # We need the full history (and main branch) to generate the report.
          fetch-depth: 0
      - name: Downloading compilation benchmark results
        run: |
          gcloud storage cp \
            "${COMPILE_STATS_RESULTS_GCS_ARTIFACT}" \
            "${COMPILE_STATS_RESULTS}"
      - name: Downloading execution benchmark results
        id: download-execution-results
        # Skip the download if there is no execution benchmark results (e.g. no
        # benchmark matches the preset/filter). In such case, no benchmark job
        # is run in benchmark_execution.yml and the output variables are empty.
        if: env.EXECUTION_BENCHMARK_RESULTS_GCS_ARTIFACT_DIR != ''
        run: |
          gcloud storage cp -r \
            "${EXECUTION_BENCHMARK_RESULTS_GCS_ARTIFACT_DIR}/benchmark-results-*.json" \
            "${EXECUTION_BENCHMARK_RESULTS_DIR}"
          echo "execution-benchmark-results-pattern=${EXECUTION_BENCHMARK_RESULTS_DIR}/benchmark-results-*.json" >> "${GITHUB_OUTPUT}"
      - name: Generating comment
        if: inputs.ci-stage == 'presubmit'
        id: generate-comment
        env:
          # Wildcard pattern to match all execution benchmark results. Empty if
          # execution_benchmarks is skipped, which results in no match.
          EXECUTION_BENCHMARK_RESULTS_PATTERN: ${{ steps.download-execution-results.outputs.execution-benchmark-results-pattern }}
          IREE_BUILD_URL: https://github.com/openxla/iree/actions/runs/${{ github.run_id }}/attempts/${{ github.run_attempt }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BENCHMARK_COMMENT_ARTIFACT: benchmark-comment.json
        run: |
          build_tools/github_actions/docker_run.sh \
            gcr.io/iree-oss/benchmark-report@sha256:7498c6f32f63f13faf085463cc38656d4297519c824e63e1c99c8c258147f6ff \
            ./build_tools/benchmarks/generate_benchmark_comment.py \
              --verbose \
              --pr_number="${PR_NUMBER}" \
              --pr_committish="${GITHUB_SHA}" \
              --pr_base_branch="origin/${GITHUB_BASE_REF}" \
              --comment_type="benchmark-summary" \
              --build_url="${IREE_BUILD_URL}" \
              --benchmark_files="${EXECUTION_BENCHMARK_RESULTS_PATTERN}" \
              --compile_stats_files="${COMPILE_STATS_RESULTS}" \
              --output="${BENCHMARK_COMMENT_ARTIFACT}"
          echo "benchmark-comment-artifact=${BENCHMARK_COMMENT_ARTIFACT}" >> "${GITHUB_OUTPUT}"
      - name: Uploading comment artifact
        # Due to security reasons, instead of posting the comment to PR, we only
        # upload the comment data in presubmit workflow and trigger the posting
        # workflow on the main branch. See post_benchmark_comment.yaml
        if: inputs.ci-stage == 'presubmit'
        env:
          BENCHMARK_COMMENT_ARTIFACT: ${{ steps.generate-comment.outputs.benchmark-comment-artifact }}
          BENCHMARK_COMMENT_GCS_ARTIFACT: ${{ env.GCS_DIR }}/${{ steps.generate-comment.outputs.benchmark-comment-artifact }}
        run: |
          gcloud storage cp \
            "${BENCHMARK_COMMENT_ARTIFACT}" \
            "${BENCHMARK_COMMENT_GCS_ARTIFACT}"
          echo "::notice title=JOB_OUTPUT_VARIABLE::benchmark-comment-gcs-artifact=${BENCHMARK_COMMENT_GCS_ARTIFACT}"
      - name: Uploading results to dashboard
        if: inputs.ci-stage == 'postsubmit'
        env:
          EXECUTION_BENCHMARK_RESULTS_PATTERN: ${{ steps.download-execution-results.outputs.execution-benchmark-results-pattern }}
          IREE_DASHBOARD_API_TOKEN: ${{ secrets.IREE_DASHBOARD_API_TOKEN }}
        run: |
          build_tools/github_actions/docker_run.sh \
            --env "IREE_DASHBOARD_API_TOKEN=${IREE_DASHBOARD_API_TOKEN}" \
            gcr.io/iree-oss/benchmark-report@sha256:7498c6f32f63f13faf085463cc38656d4297519c824e63e1c99c8c258147f6ff \
            ./build_tools/benchmarks/upload_benchmarks_to_dashboard.py \
              --verbose \
              --benchmark_files="${EXECUTION_BENCHMARK_RESULTS_PATTERN}" \
              --compile_stats_files="${COMPILE_STATS_RESULTS}"
