# IREE Benchmark Suites

**For working with the legacy benchmark suites, see [IREE Benchmark Suites Tool (Legacy)](#iree-benchmark-suites-tool-legacy)**

This directory contains the tools to run IREE benchmark suites and generate
reports. Information about the configrations of benchmark suites can be found
[here](/build_tools/python/benchmark_suites/iree/README.md).

We are in progress to replace the [legacy cmake benchmark suites](/benchmarks)
with a new one written in python. Currently the tools only support `x86_64`,
`CUDA`, and `compilation statistics` benchmarks.

**Our benchmark CI (https://perf.iree.dev) is using the new benchmark suites and
tools to benchmark those targets.**

## Running Benchmark Suites Locally

### Prerequisites
- Install `iree-import-tf` and `iree-import-tflite` in your Python environment
  (see [Tensorflow Integration](https://openxla.github.io/iree/getting-started/tensorflow/)
  and [TFLite Integration](https://openxla.github.io/iree/getting-started/tflite/)).

### Build Benchmark Suites
Configure IREE with `-DIREE_BUILD_E2E_TEST_ARTIFACTS=ON`:
```sh
cmake -GNinja -B "${IREE_BUILD_DIR}" -S "${IREE_REPO}" \
  -DCMAKE_BUILD_TYPE=RelWithDebInfo \
  -DCMAKE_C_COMPILER=clang \
  -DCMAKE_CXX_COMPILER=clang++ \
  -DIREE_ENABLE_LLD=ON \
  -DIREE_BUILD_E2E_TEST_ARTIFACTS=ON
```
Build the benchmark suites and tools:
```sh
cmake --build "${IREE_BUILD_DIR}" --target \
  iree-e2e-test-artifacts \
  iree-benchmark-module
```

### Run Benchmarks
Export the execution benchmark config:
```sh
build_tools/benchmarks/export_benchmark_config.py execution > exec_config.json
```
Run benchmarks (currently only support running on a Linux host):
```sh
build_tools/benchmarks/run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir="${IREE_BUILD_DIR}/tools" \
  --e2e_test_artifacts_dir="${IREE_BUILD_DIR}/e2e_test_artifacts" \
  --execution_benchmark_config=exec_config.json \
  --target_device_name="<target_device_name, e.g. c2-standard-16>" \
  --output=benchmark_results.json \
  --verbose \
  --cpu_uarch="<host CPU uarch, e.g. CascadeLake>"
# Traces can be collected by adding:
# --traced_benchmark_tool_dir="${IREE_TRACED_BUILD_DIR}/tools" \
# --trace_capture_tool=/path/to/iree-tracy-capture \
# --capture_tarball=captured_tracy_files.tar.gz
```
Note that:
- `<target_device_name>` selects a benchmark group targets a specific device:
  - Common options:
    - `c2-standard-16` for x86_64 CPU benchmarks
    - `a2-highgpu-1g` for NVIDIA GPU benchmarks
  - All device names are defined [here](/build_tools/python/e2e_test_framework/device_specs)
- To run x86_64 benchmarks, right now `--cpu_uarch` needs to be provided and
  only `CascadeLake` is available currently.
- To build traced benchmark tools, see instructions
  [here](/docs/developers/developing_iree/profiling_with_tracy.md).

Filters can be used to select the benchmarks:
```sh
build_tools/benchmarks/run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir="${IREE_BUILD_DIR}/tools" \
  --e2e_test_artifacts_dir="${IREE_BUILD_DIR}/e2e_test_artifacts" \
  --execution_benchmark_config=exec_config.json \
  --target_device_name="c2-standard-16" \
  --output=benchmark_results.json \
  --verbose \
  --cpu_uarch="CascadeLake" \
  --model_name_regex="MobileBert*" \
  --driver_filter_regex='local-task' \
  --mode_regex="4-thread"
```

### Generate Compilation Statistics (Compilation Benchmarks)
Export the compilation benchmark config:
```sh
build_tools/benchmarks/export_benchmark_config.py compilation > comp_config.json
```
Generate the compilation statistics:
```sh
build_tools/benchmarks/collect_compilation_statistics.py \
  alpha \
  --compilation_benchmark_config=comp_config.json \
  --e2e_test_artifacts_dir="${IREE_BUILD_DIR}/e2e_test_artifacts" \
  --build_log="${IREE_BUILD_DIR}/.ninja_log" \
  --output=compile_stats_results.json
```
Note that you need to use **ninja** to build the benchmark suites as the tool
collects information from its build log.

### Show Execution / Compilation Benchmark Results
See the section [Generating Benchmark Report](#generating-benchmark-report)

### Find Compile and Run Commands to Reproduce Benchmarks
Each benchmark has its benchmark ID in the benchmark suites, you will see their
benchmark IDs on:
- On https://perf.iree.dev, each serie's URL is in the format:
  - Execution benchmark: `https://perf.iree.dev/serie?IREE?<benchmark_id>`
  - Compilation benchmark: `https://perf.iree.dev/serie?IREE?<benchmark_id>-<metric_id>`
- In `benchmark_results.json` and `compile_stats_results.json`
  - Each execution benchmark result has a field `run_config_id`
  - Each compilation benchmark result has a field `gen_config_id`
- In the markdown generated by `diff_local_benchmarks.py`, each benchmark shows
  its https://perf.iree.dev URL, which contains its benchmark ID.

Run the helper tool to dump all commands from benchmark configs:
```sh
build_tools/benchmarks/benchmark_helper.py dump-cmds \
  --execution_benchmark_config=exec_config.json \
  --compilation_benchmark_config=comp_config.json \
  --benchmark_id="<benchmark ID>"
```
> TODO(#12215): Dump should also include searchable benchmark names.

## Fetching Benchmark Artifacts from CI

##### 1. Find the corresponding CI workflow run
On the commit of the benchmark run, you can find the list of the workflow jobs
by clicking the green check mark. Click the **Details** of job `build_e2e_test_artifacts`:

![image](https://user-images.githubusercontent.com/2104162/223781032-c22e2922-2bd7-422d-abc2-d6ef0d31b0f8.png)

##### 2. Get the GCS directory of benchmark artifacts
On the job detail page, expand the step Uploading e2 test artifacts, you will
see a bunch of lines like:
```
Copying file://build-e2e-test-artifacts/e2e_test_artifacts/iree_MobileBertSquad_fp32_module_fdff4caa105318036534bd28b76a6fe34e6e2412752c1a000f50fafe7f01ef07/module.vmfb to gs://iree-github-actions-postsubmit-artifacts/4360950546/1/e2e-test-artifacts/iree_MobileBertSquad_fp32_module_fdff4caa105318036534bd28b76a6fe34e6e2412752c1a000f50fafe7f01ef07/module.vmfb
...
```
The URL `gs://iree-github-actions-...-artifacts/.../.../e2e-test-artifacts/` is
the GCS directory of benchmark artifacts.

> TODO(#12215): Show the GCS URL in the workflow summary page.

##### 3. Fetch the benchmark artifacts
You can use [gcloud CLI tool](https://cloud.google.com/sdk/docs/install) to list
the directoy contents (see the [doc](https://cloud.google.com/sdk/gcloud/reference/storage)
for more usages):
```sh
gcloud storage ls gs://iree-github-actions-postsubmit-artifacts/.../.../e2e-test-artifacts
```
The GCS directory has the same structure as your local `"$IREE_BUILD_DIR/e2e_test_artifacts"`.

Execution and compilation benchmark configs can be downloaded at:
```sh
# Execution benchmark config:
gcloud storage cp gs://iree-github-actions-postsubmit-artifacts/.../.../benchmark-config.json
# Compilation benchmark config:
gcloud storage cp gs://iree-github-actions-postsubmit-artifacts/.../.../compilation-config.json
```

# IREE Benchmark Suites Tool (Legacy)

**For working with the new benchmark suite, see [IREE New Benchmark Suite](#iree-new-benchmark-suite)**

## Benchmark Tools

Currently we have `run_benchmarks_on_android.py` and
`run_benchmarks_on_linux.py` scripts to run benchmark suites on Android devices
(with `adb`) and Linux machines.

The available arguments can be shown with `--help`. Some common usages are
listed below. Here we assume:

```sh
IREE_BUILD_DIR="/path/to/IREE build root dir". It should contain the "benchmark_suites" directory built with the target "iree-benchmark-suites".

IREE_NORMAL_TOOL_DIR="/path/to/IREE tool dir". It is usually "$IREE_BUILD_DIR/tools".

IREE_TRACED_TOOL_DIR="/path/to/IREE tool dir built with IREE_ENABLE_RUNTIME_TRACING=ON".
```

See details about `IREE_ENABLE_RUNTIME_TRACING` [here](/docs/developers/developing_iree/profiling_with_tracy.md).

**Run all benchmarks**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --output=results.json $IREE_BUILD_DIR
```

**Run all benchmarks and perform the Tracy captures**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --traced_benchmark_tool_dir=$IREE_TRACED_TOOL_DIR \
  --trace_capture_tool=/path/to/iree-tracy-capture \
  --capture_tarball=captured_tracy_files.tar.gz
  --output=results.json $IREE_BUILD_DIR
```

**Run selected benchmarks with the filters**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --model_name_regex="MobileBertSquad" \
  --driver_filter_regex="local-task" \
  --mode_regex="4-threads" \
  --output=results.json $IREE_BUILD_DIR
```

**Collect compilation statistics**

See [here](/benchmarks/README.md#collect-compile-stats) for additional build
steps to enable compilation statistics collection.
```sh
./collect_compilation_statistics.py \
  legacy \
  --output "compile-stats.json" \
  "${IREE_BUILD_DIR}"
```

## Generating Benchmark Report

The tools here are mainly designed for benchmark automation pipelines.
The `post_benchmarks_as_pr_comment.py` and `upload_benchmarks_to_dashboard.py`
scripts are used to upload and post reports to pull requests or the
[dashboard](https://perf.iree.dev/).

If you want to generate a comparison report locally, you can use
`diff_local_benchmarks.py` script to compare two result json files and generate
the report. For example:

```sh
./diff_local_benchmarks.py --base before.json --target after.json > report.md
```

An example that compares compilation statistics:

```sh
./diff_local_benchmarks.py \
  --base-compile-stats "compile-stats-before.json" \
  --target-compile-stats "compile-stats-after.json" \
  > report.md
```
