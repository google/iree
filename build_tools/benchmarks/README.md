# IREE New Benchmark Suite

**For working with the legacy benchmark suite, see [IREE Benchmark Suites Tool (Legacy)](#iree-benchmark-suites-tool-legacy)**

We are in progress to replace the [legacy cmake benchmark suite](/benchmarks)
with a new one written in python. Currently it only supports `x86_64`, `CUDA`,
and `compilation statistics` benchmarks.

**Our benchmark CI (https://perf.iree.dev) is using the new benchmark suite to
benchmark those targets.**

## Running Benchmark Suite Locally 

### Prerequisites
- Install `iree-import-tf` and `iree-import-tflite` in your Python environment
  (see [Tensorflow Integration](https://openxla.github.io/iree/getting-started/tensorflow/)
  and [TFLite Integration](https://openxla.github.io/iree/getting-started/tflite/)).
- Install `jq` to manipulate JSON in command lines (recommended).

### Build Benchmark Suite
Configure IREE with `-DIREE_BUILD_E2E_TEST_ARTIFACTS=ON`:
```sh
cmake -GNinja -B "${IREE_BUILD_DIR}" -S "${IREE_REPO}" \
  -DCMAKE_BUILD_TYPE=RelWithDebInfo \
  -DCMAKE_C_COMPILER=clang \
  -DCMAKE_CXX_COMPILER=clang++ \
  -DIREE_ENABLE_LLD=ON \
  -DIREE_BUILD_E2E_TEST_ARTIFACTS=ON
```
Then build the benchmark suites and tools:
```sh
cmake --build "${IREE_BUILD_DIR}" --target \
  iree-e2e-test-artifacts \
  iree-benchmark-module
```

### Run Benchmarks
First export the JSON benchmark config (defines what benchmarks to run):
```sh
build_tools/benchmarks/export_benchmark_config.py execution > exec_config.json
```
Then run benchmarks (currently only support running on a Linux host):
```sh
build_tools/benchmarks/run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir="$IREE_BUILD_DIR/tools" \
  --e2e_test_artifacts_dir="$IREE_BUILD_DIR/e2e_test_artifacts" \
  --execution_benchmark_config=exec_config.json \
  --target_device_name="<target_device_name, e.g. c2-standard-16>" \
  --output=benchmark_results.json \
  --verbose \
  --cpu_uarch="<Host CPU uarch, e.g. CascadeLake>"
# Traces can be collected by adding:
# --traced_benchmark_tool_dir="$IREE_TRACED_BUILD_DIR/tools" \
# --trace_capture_tool=/path/to/iree-tracy-capture \
# --capture_tarball=captured_tracy_files.tar.gz
```
Note that:
- Benchmarks are grouped by their target device and `<target_device_name>`
  selects which group to run.
  - Common options:
    - `c2-standard-16` for x86_64 CPU benchmarks
    - `a2-highgpu-1g` for NVIDIA GPU benchmarks
  - All device names are defined [here](/build_tools/python/e2e_test_framework/device_specs)
- To run x86_64 benchmarks, right now `--cpu_uarch` needs to be provided and
  only `CascadeLake` is available currently.
- To build traced benchmark tools, see the instructions [here](/docs/developers/developing_iree/profiling_with_tracy.md).

Filters can be used to select the benchmarks:
```sh
build_tools/benchmarks/run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir="$IREE_BUILD_DIR/tools" \
  --e2e_test_artifacts_dir="$IREE_BUILD_DIR/e2e_test_artifacts" \
  --execution_benchmark_config=exec_config.json \
  --target_device_name="c2-standard-16" \
  --output=benchmark_results.json \
  --verbose \
  --cpu_uarch="CascadeLake" \
  --model_name_regex="MobileBert*" \
  --driver_filter_regex='local-task' \
  --mode_regex="4-thread"
```

### Generate Compilation Statistics (compilation benchmarks)
First export the compilation benchmark config:
```sh
build_tools/benchmarks/export_benchmark_config.py compilation > comp_config.json
```
Generate the compilation statistics:
```sh
build_tools/benchmarks/collect_compilation_statistics.py \
  alpha \
  --compilation_benchmark_config=comp_config.json \
  --e2e_test_artifacts_dir="${IREE_BUILD_DIR}/e2e_test_artifacts" \
  --build_log="${IREE_BUILD_DIR}/.ninja_log" \
  --output=compile_stats_results.json
```

### Show Execution / Compilation Benchmark Results
See the section [Generating Benchmark Report](#generating-benchmark-report)

### Find Compile and Run Flags of Benchmarks
Each benchmark has its permanent benchmark id in the benchmark suite, you can
find their benchmark ids from:
- On https://perf.iree.dev, each serie's URL is in the format:
  - Execution benchmark: `https://perf.iree.dev/serie?IREE?<benchmark_id>`
  - Compilation benchmark: `https://perf.iree.dev/serie?IREE?<benchmark_id>-<metric_id>`
- In `benchmark_results.json` and `compile_stats_results.json`
  - Each execution benchmark has a field `run_config_id`
  - Each compilation benchmark has a field `gen_config_id`
- In the markdown generated by `diff_local_benchmarks.py`, each benchmark shows
  its https://perf.iree.dev URL, which contains its benchmark id.

We refer the execution benchmark id as `${EXEC_BENCHMARK_ID}` and the
compilation benchmark id as `${COMP_BENCHMARK_ID}` below.

#### Get the compile and run flags

We provide a tool `benchmark_helper.py` to dump the compile and run flags of
benchmarks. See the examples below:

##### Get run and compile flags of an execution benchmark
```sh
./build_tools/benchmarks/benchmark_helper.py dump-flags \
  --execution_benchmark_config exec_config.json \
  | jq --arg exec_id "${EXEC_BENCHMARK_ID}" '[.[]] | add | .[$exec_id]'
```
Example output:
```json
{
  "composite_id": "fcc2eb7748902acc86b82e71de537c9f38bd0baccb9ff8da2688a806278116a0",
  "run_flags": [
    "--function=main",
    "--input=1x257x257x3xf32=0",
    "--device_allocator=caching",
    "--device=local-sync",
    "--module=iree_DeepLabV3_fp32_module_87aead729018ce5f114501cecefb6315086eb2a21ae1b30984b1794f619871c6/module.vmfb"
  ],
  "module_generation_config": {
    "composite_id": "87aead729018ce5f114501cecefb6315086eb2a21ae1b30984b1794f619871c6",
    "compile_flags": [
      "--iree-hal-target-backends=llvm-cpu",
      "--iree-input-type=tosa",
      "--iree-llvm-target-triple=x86_64-unknown-linux-gnu",
      "--iree-llvm-target-cpu=cascadelake",
      "iree_DeepLabV3_fp32_05c50f54ffea1fce722d07588e7de026ce10324eccc5d83d1eac2c5a9f5d639d.mlir"
    ],
    "import_tool": "iree-import-tflite",
    "import_flags": [
      "--output-format=mlir-bytecode",
      "model_c36c63b0-220a-4d78-8ade-c45ce47d89d3_DeepLabV3_fp32.tflite"
    ]
  }
}
```
##### Get compile flags of a compilation benchmark
```sh
./build_tools/benchmarks/benchmark_helper.py dump-flags \
  --compilation_benchmark_config comp_config.json \
  | jq --arg comp_id "${COMP_BENCHMARK_ID}" '.[$comp_id]'
```
Example output:
```json
{
  "composite_id": "178907b155b6322dedfa947937f9caca5158ff3af167470f2de90347dba357f4",
  "compile_flags": [
    "--iree-hal-target-backends=vulkan-spirv",
    "--iree-input-type=tosa",
    "--iree-vulkan-target-triple=valhall-unknown-android31",
    "--iree-flow-enable-fuse-padding-into-linalg-consumer-ops",
    "--iree-hal-benchmark-dispatch-repeat-count=32",
    "--iree-vm-emit-polyglot-zip=true",
    "--iree-llvm-debug-symbols=false",
    "iree_DeepLabV3_fp32_05c50f54ffea1fce722d07588e7de026ce10324eccc5d83d1eac2c5a9f5d639d.mlir"
  ],
  "import_tool": "iree-import-tflite",
  "import_flags": [
    "--output-format=mlir-bytecode",
    "model_c36c63b0-220a-4d78-8ade-c45ce47d89d3_DeepLabV3_fp32.tflite"
  ]
}
```

### Exploring Benchmark Config

If you are interested in more detailed benchmark information, all metadata about
a benchmark is stored in the exported benchmark config.

The config file is a JSON containing serialized Python objects in the format:
```json
# Execution Benchmark Config
{
  "c2-standard-16": {
    "run_configs": [
      {
        "root_objs": [...],
        "keyed_obj_map": {...}
      },
      ...
    ],
    "module_dir_paths": [...],
    "host_environment": {...},
  },
  "a2-highgpu-1g": {
    ...
  },
  ...
}

# Compilation Benchmark Config
{
  "generation_configs": [
    {
      "root_objs": [...],
      "keyed_obj_map": {...}
    },
    ... 
  ],
  "module_dir_paths": [...],
}
```
The `run_configs` and `generation_configs` are the places stored the actual
benchmark definitions. In their `keyed_obj_map`, there are 4 kinds of important
objects listed below:

> The contents are not easy to understand because the serializer serializes the
objects into `keyed_obj_map` and uses their ids elsewhere to reference them.
> Check [build_tools/python/e2e_test_framework/definitions/iree_definitions.py](/build_tools/python/e2e_test_framework/definitions/iree_definitions.py)
> for all object definitions in Python.

> TODO(#12215): Improve the readability of the serialized JSON config.

```json
"iree_e2e_model_run_configs:<execution_benchmark_id>": {
  "module_generation_config": <id to its module generation config>
  "run_flags": [...]
}

"iree_module_generation_configs:<compilation_benchmark_id>": {
  "imported_model": <id to its imported model>
  "compile_flags": [...]
}

"iree_imported_models:<imported_model_id>": {
  "model": <id to its model>
}

"models:<model_id>": {
  "name": <model name>,
  ...
}
```

In order to get the flags, artifacts, and metadata, you will need to walk
through these objects, starting with either
`iree_e2e_model_run_configs:${EXEC_BENCHMARK_ID}` or
`iree_module_generation_configs:${COMP_BENCHMARK_ID}`. Here are some useful
commands to extract the information:

##### Get information of an execution benchmark
Searching `iree_e2e_model_run_configs:${EXEC_BENCHMARK_ID}` in
`exec_config.json`, you can find an `E2EModelRunConfig` object like:
```json
"iree_e2e_model_run_configs:e496c2ea8de7fdffdb7597da63eed12cc5fb0595605d70e1f9d67a33857a299a": {
  "composite_id": "e496c2ea8de7fdffdb7597da63eed12cc5fb0595605d70e1f9d67a33857a299a",
  // ModuleGenerationConfig id
  "module_generation_config": "7a0add4835462bc66025022cdb6e87569da79cf103825a809863b8bd57a49055",
  // ModuleExecutionConfig id
  "module_execution_config": "13fc65a9-e5dc-4cbb-9c09-25b0b08f4c03",
  // TargetDeviceSpec id
  "target_device_spec": "9a4804f1-b1b9-46cd-b251-7f16a655f782",
  // ModelInputData id
  "input_data": "8d4a034e-944d-4725-8402-d6f6e61be93c",
  // All run flags
  "run_flags": [
    "--function=forward",
    "--input=1x224x224x3xf32=0",
    "--device_allocator=caching",
    "--device=local-sync"
  ]
}
```

With the module generation config id, you can search
`iree_module_generation_configs:<module generation config id>` and find a
`ModuleGenerationConfig` object like:
```json
"iree_module_generation_configs:7a0add4835462bc66025022cdb6e87569da79cf103825a809863b8bd57a49055": {
  "composite_id": "7a0add4835462bc66025022cdb6e87569da79cf103825a809863b8bd57a49055",
  // ImportedModel id
  "imported_model": "a122dabcac56c201a4c98d3474265f15adba14bff88353f421b1a11cadcdea1f",
  // CompileConfig id
  "compile_config": "e7e18b0f-c72d-4f1c-89b1-5afee70df6e9",
  // All compile flags
  "compile_flags": [
    "--iree-hal-target-backends=llvm-cpu",
    "--iree-input-type=mhlo",
    "--iree-llvm-target-triple=x86_64-unknown-linux-gnu",
    "--iree-llvm-target-cpu=cascadelake"
  ]
}
```

With the imported model id, you can search
`iree_imported_models:<imported model id>` and find a `ImportedModel` object
like:
```json
"iree_imported_models:a122dabcac56c201a4c98d3474265f15adba14bff88353f421b1a11cadcdea1f": {
  "composite_id": "a122dabcac56c201a4c98d3474265f15adba14bff88353f421b1a11cadcdea1f",
  // Model id
  "model": "c393b4fa-beb4-45d5-982a-c6328aa05d08",
  // ImportConfig id
  "import_config": "8b2df698-f3ba-4207-8696-6c909776eac4"
}
```

You can find the related artifacts with these ids:
```sh
# VMFB module:
${IREE_BUILD_DIR}/e2e_test_artifacts/iree_*_<ModuleGenerationConfig id>/module.vmfb

# Imported MLIR (might not exist if the input model is already in MLIR):
${IREE_BUILD_DIR}/e2e_test_artifacts/iree_*_<ImportedModel id>.mlir

# Input model:
${IREE_BUILD_DIR}/e2e_test_artifacts/model_<Model id>_*
```

##### Get information of an compilation benchmark
Same as [Get information of an execution benchmark](#get-information-of-an-execution-benchmark),
but you start by searching `iree_module_generation_configs:${COMP_BENCHMARK_ID}`
to get the `ModuleGenerationConfig`.


## Finding and Manipulating Benchmarks
All benchmarks are defined by the scripts under
[build_tools/python/benchmark_suites/iree](/build_tools/python/benchmark_suites/iree).

To find the code that generates a benchmark:
1. Get the ids of `CompileConfig` and `ModuleExecutionConfig`.
   - If you have a benchmark id, see [Get information of an execution benchmark](#get-information-of-an-execution-benchmark)
   or [Get information of an compilation benchmark](#get-information-of-an-compilation-benchmark)
   for how to find them in an exported benchmark config.
2. Find the constants of these ids in [build_tools/python/e2e_test_framework/unique_ids.py](/build_tools/python/e2e_test_framework/unique_ids.py).
3. Search in the code to see which generator uses those constants.
   - `CompileConfig` id should help you locale which script generates it, as it
   indicates the target architecture.

> TODO(#12215): Add a doc to explain how to hack the benchmark suite.

To manipulate the benchmarks:
1. Modify the benchmark generation code under
   [build_tools/python/benchmark_suites/iree](/build_tools/python/benchmark_suites/iree).
2. Follow
   [tests/e2e/test_artifacts](https://github.com/openxla/iree/tree/main/tests/e2e/test_artifacts)
   to regenerate the cmake files.
3. Rebuild the benchmark suite.
4. Make sure to export the new benchmark configs again before running the
   benchmarks.

## Fetching Benchmark Artifacts from CI

##### 1. Find the corresponding CI workflow run
On the commit of a benchmark run, you can find the list of the workflow jobs by
clicking the green check mark. Click the **Details** of job
`build_e2e_test_artifacts`:

![image](https://user-images.githubusercontent.com/2104162/223781032-c22e2922-2bd7-422d-abc2-d6ef0d31b0f8.png)

##### 2. Get GCS directory of benchmark artifacts
On the job detail page, expand the step Uploading e2 test artifacts, you will
see a bunch of lines like:
```
Copying file://build-e2e-test-artifacts/e2e_test_artifacts/iree_MobileBertSquad_fp32_module_fdff4caa105318036534bd28b76a6fe34e6e2412752c1a000f50fafe7f01ef07/module.vmfb to gs://iree-github-actions-postsubmit-artifacts/4360950546/1/e2e-test-artifacts/iree_MobileBertSquad_fp32_module_fdff4caa105318036534bd28b76a6fe34e6e2412752c1a000f50fafe7f01ef07/module.vmfb
...
```
`gs://iree-github-actions-...-artifacts/.../.../e2e-test-artifacts/` is the GCS
directory of benchmark artifacts.

You can use `gcloud` tool to list the contents (see its [doc](https://cloud.google.com/sdk/gcloud/reference/storage)
for more usages):
```sh
gcloud storage ls gs://iree-github-actions-postsubmit-artifacts/4360950546/1/e2e-test-artifacts
```
The GCS directory has the same structure as your local `"$IREE_BUILD_DIR/e2e_test_artifacts"`.

# IREE Benchmark Suites Tool (Legacy)

**For working with the new benchmark suite, see [IREE New Benchmark Suite](#iree-new-benchmark-suite)**

This directory contains the tools to run IREE benchmark suites and generate
reports. More information about benchmark suites can be found [here](/benchmarks/README.md).

## Benchmark Tools

Currently we have `run_benchmarks_on_android.py` and
`run_benchmarks_on_linux.py` scripts to run benchmark suites on Android devices
(with `adb`) and Linux machines.

The available arguments can be shown with `--help`. Some common usages are
listed below. Here we assume:

```sh
IREE_BUILD_DIR="/path/to/IREE build root dir". It should contain the "benchmark_suites" directory built with the target "iree-benchmark-suites".

IREE_NORMAL_TOOL_DIR="/path/to/IREE tool dir". It is usually "$IREE_BUILD_DIR/tools".

IREE_TRACED_TOOL_DIR="/path/to/IREE tool dir built with IREE_ENABLE_RUNTIME_TRACING=ON".
```

See details about `IREE_ENABLE_RUNTIME_TRACING` [here](/docs/developers/developing_iree/profiling_with_tracy.md).

**Run all benchmarks**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --output=results.json $IREE_BUILD_DIR
```

**Run all benchmarks and perform the Tracy captures**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --traced_benchmark_tool_dir=$IREE_TRACED_TOOL_DIR \
  --trace_capture_tool=/path/to/iree-tracy-capture \
  --capture_tarball=captured_tracy_files.tar.gz
  --output=results.json $IREE_BUILD_DIR
```

**Run selected benchmarks with the filters**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --model_name_regex="MobileBertSquad" \
  --driver_filter_regex="local-task" \
  --mode_regex="4-threads" \
  --output=results.json $IREE_BUILD_DIR
```

**Collect compilation statistics**

See [here](/benchmarks/README.md#collect-compile-stats) for additional build
steps to enable compilation statistics collection.
```sh
./collect_compilation_statistics.py \
  legacy \
  --output "compile-stats.json" \
  "${IREE_BUILD_DIR}"
```

## Generating Benchmark Report

The tools here are mainly designed for benchmark automation pipelines.
The `post_benchmarks_as_pr_comment.py` and `upload_benchmarks_to_dashboard.py`
scripts are used to upload and post reports to pull requests or the
[dashboard](https://perf.iree.dev/).

If you want to generate a comparison report locally, you can use
`diff_local_benchmarks.py` script to compare two result json files and generate
the report. For example:

```sh
./diff_local_benchmarks.py --base before.json --target after.json > report.md
```

An example that compares compilation statistics:

```sh
./diff_local_benchmarks.py \
  --base-compile-stats "compile-stats-before.json" \
  --target-compile-stats "compile-stats-after.json" \
  > report.md
```
