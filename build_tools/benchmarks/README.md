# IREE New Benchmark Suite

**For working with the legacy benchmark suite, see [IREE Benchmark Suites Tool (Legacy)](#iree-benchmark-suites-tool-legacy)**

We are in progress to replace the [legacy cmake benchmark suite](/benchmarks)
with a new one written in python. Currently it only supports `x86_64`, `CUDA`,
and `compilation statistics` benchmarks.

**Our benchmark CI (https://perf.iree.dev) is using the new benchmark suite to
benchmark those targets.**

## Running Benchmark Suite Locally 

### Prerequisites
- Install `iree-import-tf` and `iree-import-tflite` in your Python environment
  (see [Tensorflow Integration](https://openxla.github.io/iree/getting-started/tensorflow/)
  and [TFLite Integration](https://openxla.github.io/iree/getting-started/tflite/)).
- Install `jq` to manipulate JSON in command lines (recommended).

### Build Benchmark Suite
Configure IREE with `-DIREE_BUILD_E2E_TEST_ARTIFACTS=ON`:
```sh
cmake -GNinja -B "${IREE_BUILD_DIR}" -S "${IREE_REPO}" \
  -DCMAKE_BUILD_TYPE=RelWithDebInfo \
  -DCMAKE_C_COMPILER=clang \
  -DCMAKE_CXX_COMPILER=clang++ \
  -DIREE_ENABLE_LLD=ON \
  -DIREE_BUILD_E2E_TEST_ARTIFACTS=ON
```
Then build the benchmark suites and tools:
```sh
cmake --build "${IREE_BUILD_DIR}" --target \
  iree-e2e-test-artifacts \
  iree-benchmark-module
```

### Run Benchmarks
First export the JSON benchmark config (defines what benchmarks to run):
```sh
build_tools/benchmarks/export_benchmark_config.py execution > exec_config.json
```
Then run benchmarks (currently only support running on a Linux host):
```sh
build_tools/benchmarks/run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir="$IREE_BUILD_DIR/tools" \
  --e2e_test_artifacts_dir="$IREE_BUILD_DIR/e2e_test_artifacts" \
  --execution_benchmark_config=exec_config.json \
  --target_device_name="<target_device_name, e.g. c2-standard-16>" \
  --output=benchmark_results.json \
  --verbose \
  --cpu_uarch="<CPU uarch, e.g. CascadeLake>"
# Traces can be collected by adding:
# --traced_benchmark_tool_dir="$IREE_TRACED_BUILD_DIR/tools" \
# --trace_capture_tool=/path/to/iree-tracy-capture \
# --capture_tarball=captured_tracy_files.tar.gz
```
Note that:
- Benchmarks are grouped by their target device and `<target_device_name>`
  selects which group to run.
  - Common options:
    - `c2-standard-16` for x86_64 CPU benchmarks
    - `a2-highgpu-1g` for NVIDIA GPU benchmarks
  - All device names are defined [here](/build_tools/python/e2e_test_framework/device_specs)
- To run x86_64 benchmarks, right now `--cpu_uarch` needs to be provided and
  only `CascadeLake` is available currently.
- To build traced benchmark tools, see the instructions [here](/docs/developers/developing_iree/profiling_with_tracy.md).

Filters can be used to select the benchmarks:
```sh
build_tools/benchmarks/run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir="$IREE_BUILD_DIR/tools" \
  --e2e_test_artifacts_dir="$IREE_BUILD_DIR/e2e_test_artifacts" \
  --execution_benchmark_config=exec_config.json \
  --target_device_name="c2-standard-16" \
  --output=benchmark_results.json \
  --verbose \
  --cpu_uarch="CascadeLake" \
  --model_name_regex="MobileBert*" \
  --driver_filter_regex='local-task' \
  --mode_regex="4-thread"
```

### Generate Compilation Statistics (compilation benchmarks)
First export the compilation benchmark config:
```sh
build_tools/benchmarks/export_benchmark_config.py compilation > comp_config.json
```
Generate the compilation statistics:
```sh
build_tools/benchmarks/collect_compilation_statistics.py \
  alpha \
  --compilation_benchmark_config=comp_config.json \
  --e2e_test_artifacts_dir="${IREE_BUILD_DIR}/e2e_test_artifacts" \
  --build_log="${IREE_BUILD_DIR}/.ninja_log" \
  --output=compile_stats_results.json
```

### Show Execution / Compilation Benchmark Results
See the section [Generating Benchmark Report](#generating-benchmark-report)

### Find Compile and Run Flags of Benchmarks
Each benchmark has its permanent benchmark id in the benchmark suite, you can
find their benchmark ids from:
- On https://perf.iree.dev, each serie's URL is in the format:
  - Execution benchmark: `https://perf.iree.dev/serie?IREE?<benchmark_id>`
  - Compilation benchmark: `https://perf.iree.dev/serie?IREE?<benchmark_id>-<metric_id>`
- In `benchmark_results.json` and `compile_stats_results.json`
  - Each execution benchmark has a field `run_config_id`
  - Each compilation benchmark has a field `gen_config_id`
- In the markdown generated by `diff_local_benchmarks.py`, each benchmark shows
  its https://perf.iree.dev URL, which contains its benchmark id.

We refer the execution benchmark id as `${EXEC_BENCHMARK_ID}` and the
compilation benchmark id as `${COMP_BENCHMARK_ID}` below.

#### Get the compile and run flags

We provide a tool `benchmark_helper.py` to dump the compile and run flags of
benchmarks. See the examples below:

##### Get run and compile flags of an execution benchmark
```sh
./build_tools/benchmarks/benchmark_helper.py dump-flags \
  --execution_benchmark_config exec_config.json \
  | jq --arg exec_id "${EXEC_BENCHMARK_ID}" '[.[]] | add | .[$exec_id]'
```
Example output:
```json
{
  "composite_id": "fcc2eb7748902acc86b82e71de537c9f38bd0baccb9ff8da2688a806278116a0",
  "run_flags": [
    "--function=main",
    "--input=1x257x257x3xf32=0",
    "--device_allocator=caching",
    "--device=local-sync",
    "--module=iree_DeepLabV3_fp32_module_87aead729018ce5f114501cecefb6315086eb2a21ae1b30984b1794f619871c6/module.vmfb"
  ],
  "module_generation_config": {
    "composite_id": "87aead729018ce5f114501cecefb6315086eb2a21ae1b30984b1794f619871c6",
    "compile_flags": [
      "--iree-hal-target-backends=llvm-cpu",
      "--iree-input-type=tosa",
      "--iree-llvm-target-triple=x86_64-unknown-linux-gnu",
      "--iree-llvm-target-cpu=cascadelake",
      "iree_DeepLabV3_fp32_05c50f54ffea1fce722d07588e7de026ce10324eccc5d83d1eac2c5a9f5d639d.mlir"
    ],
    "import_tool": "iree-import-tflite",
    "import_flags": [
      "--output-format=mlir-bytecode",
      "model_c36c63b0-220a-4d78-8ade-c45ce47d89d3_DeepLabV3_fp32.tflite"
    ]
  }
}
```
##### Get compile flags of a compilation benchmark
```sh
./build_tools/benchmarks/benchmark_helper.py dump-flags \
  --compilation_benchmark_config comp_config.json \
  | jq --arg comp_id "${COMP_BENCHMARK_ID}" '.[$comp_id]'
```
Example output:
```json
{
  "composite_id": "178907b155b6322dedfa947937f9caca5158ff3af167470f2de90347dba357f4",
  "compile_flags": [
    "--iree-hal-target-backends=vulkan-spirv",
    "--iree-input-type=tosa",
    "--iree-vulkan-target-triple=valhall-unknown-android31",
    "--iree-flow-enable-fuse-padding-into-linalg-consumer-ops",
    "--iree-hal-benchmark-dispatch-repeat-count=32",
    "--iree-vm-emit-polyglot-zip=true",
    "--iree-llvm-debug-symbols=false",
    "iree_DeepLabV3_fp32_05c50f54ffea1fce722d07588e7de026ce10324eccc5d83d1eac2c5a9f5d639d.mlir"
  ],
  "import_tool": "iree-import-tflite",
  "import_flags": [
    "--output-format=mlir-bytecode",
    "model_c36c63b0-220a-4d78-8ade-c45ce47d89d3_DeepLabV3_fp32.tflite"
  ]
}
```

### Exploring Benchmark Config

If you are interested in more detailed benchmark information, all metadata about
a benchmark is stored in the exported benchmark config.

The config file is a JSON containing serialized Python objects in the format:
```json
# Execution Benchmark Config
{
  "c2-standard-16": {
    "run_configs": [
      {
        "root_objs": [...],
        "keyed_obj_map": {...}
      },
      ...
    ],
    "module_dir_paths": [...],
    "host_environment": {...},
  },
  "a2-highgpu-1g": {
    ...
  },
  ...
}

# Compilation Benchmark Config
{
  "generation_configs": [
    {
      "root_objs": [...],
      "keyed_obj_map": {...}
    },
    ... 
  ],
  "module_dir_paths": [...],
}
```
The `run_configs` and `generation_configs` are the places stored the actual
benchmark definitions. In their `keyed_obj_map`, there are 4 kinds of important
objects listed below:

> The contents are not easy to understand because the serializer serializes the
objects into `keyed_obj_map` and uses their ids elsewhere to reference them.
> Check [build_tools/python/e2e_test_framework/definitions/iree_definitions.py](/build_tools/python/e2e_test_framework/definitions/iree_definitions.py)
> for all object definitions in Python.

```json
"iree_e2e_model_run_configs:<execution_benchmark_id>": {
  "module_generation_config": <id to its module generation config>
  "run_flags": [...]
}

"iree_module_generation_configs:<compilation_benchmark_id>": {
  "imported_model": <id to its imported model>
  "compile_flags": [...]
}

"iree_imported_models:<imported_model_id>": {
  "model": <id to its model>
}

"models:<model_id>": {
  "name": <model name>,
  ...
}
```

In order to get the flags, artifacts, and metadata, you will need to walk
through these objects, starting with either
`iree_e2e_model_run_configs:${EXEC_BENCHMARK_ID}` or
`iree_module_generation_configs:${COMP_BENCHMARK_ID}`. Here are some useful
commands to extract the information:

##### Get information of an execution benchmark
```sh
cat exec_config.json | \
  jq --arg exec_id "${EXEC_BENCHMARK_ID}" \
  'map_values(.[].keyed_obj_map? | ."iree_e2e_model_run_configs:\($exec_id)")'

This shows an e2e model run object like:
{
  "c2-standard-16": {
    "composite_id": "e496c2ea8de7fdffdb7597da63eed12cc5fb0595605d70e1f9d67a33857a299a",
    "module_generation_config": "7a0add4835462bc66025022cdb6e87569da79cf103825a809863b8bd57a49055",
    "module_execution_config": "13fc65a9-e5dc-4cbb-9c09-25b0b08f4c03",
    "target_device_spec": "9a4804f1-b1b9-46cd-b251-7f16a655f782",
    "input_data": "8d4a034e-944d-4725-8402-d6f6e61be93c",
    "run_flags": [
      "--function=forward",
      "--input=1x224x224x3xf32=0",
      "--device_allocator=caching",
      "--device=local-sync"
    ]
  },
  "a2-highgpu-1g": null,
}

Find the VMFB module at:
echo ${IREE_BUILD_DIR}/e2e_test_artifacts/iree_*_<module_generation_config>/module.vmfb
```

##### Get compilation information of an execution / compilation benchmark
First you need to get the compilation benchmark id, which can be either:

- Field `module_generation_config` of an e2e model run object from an execution
  benchmark (see [Get information of an execution benchmark](#get-information-of-an-execution-benchmark)).
- Compilation benchmark id from https://perf.iree.dev or compilation benchmark
  results.

> Note that the configs for execution and compilation benchmarks might contain
> different set of benchmark ids. Make sure you query the right config file.

```sh
# For execution benchmarks
cat exec_config.json | \
  jq --arg gen_id "${COMP_BENCHMARK_ID}" \
  'map_values(.[].keyed_obj_map? | ."iree_module_generation_configs:\($gen_id)")'

# For compilation benchmarks
cat comp_config.json | \
  jq --arg gen_id "${COMP_BENCHMARK_ID}" \
  '.[].keyed_obj_map? | ."iree_module_generation_configs:\($gen_id)"'

This shows a module generation obj like:
{
  "composite_id": "1d26fcfdb7387659356dd99ce7e10907c8560b0925ad839334b0a6155d25167a",
  "imported_model": "394878992fb35f2ed531b7f0442c05bde693346932f049cbb3614e06b3c82337",
  "compile_config": "32a56c8d-cc6c-41b8-8620-1f8eda0b8223-compile-stats",
  "compile_flags": [
    "--iree-hal-target-backends=vulkan-spirv",
    "--iree-input-type=tosa",
    "--iree-vulkan-target-triple=valhall-unknown-android31",
    "--iree-flow-enable-fuse-padding-into-linalg-consumer-ops",
    "--iree-vm-emit-polyglot-zip=true",
    "--iree-llvm-debug-symbols=false"
  ]
}

Find the VMFB module at:
echo ${IREE_BUILD_DIR}/e2e_test_artifacts/iree_*_${COMP_BENCHMARK_ID}/module.vmfb

And the input imported MLIR at:
echo ${IREE_BUILD_DIR}/e2e_test_artifacts/iree_*_<imported_model>.mlir
```

##### Get information about how the model is imported
```sh
# For execution benchmarks
cat exec_config.json | \
  jq --arg IMPORTED_ID "${IMPORTED_MODEL_ID}" \
  'map_values(.[].keyed_obj_map? | ."iree_imported_models:\($IMPORTED_ID)")'

# For compilation benchmarks
cat comp_config.json | \
  jq --arg IMPORTED_ID "${IMPORTED_MODEL_ID}" \
  '.[].keyed_obj_map? | ."iree_imported_models:\($IMPORTED_ID)"'

This shows an imported model object:
{
  "composite_id": "213fe9a8738a01f2b02b6f0614a40a31c83a2603ca3e3ae0aeab8090fedbe3a0",
  "model": "ebe7897f-5613-435b-a330-3cb967704e5e",
  "import_config": "8b2df698-f3ba-4207-8696-6c909776eac4"
}

Find the input model at:
echo ${IREE_BUILD_DIR}/e2e_test_artifacts/model_ebe7897f-5613-435b-a330-3cb967704e5e_*
```

### Find and Manipulate the Benchmark Definitions
All benchmarks are defined by the scripts under
[build_tools/python/benchmark_suites/iree](/build_tools/python/benchmark_suites/iree).

> TODO(#12215): Add a doc to explain how to hack the benchmark suite.

To find the code that generates a benchmark config:
1. Get the ids from `module_execution_config` and `compile_config` fields of the
   benchmark.
2. Find the defined constants of these ids (or their prefixes) in
   [build_tools/python/e2e_test_framework/unique_ids.py](/build_tools/python/e2e_test_framework/unique_ids.py).
3. Search in the code to see which generator uses these constants.

To manipulate the benchmarks:
1. Modify the benchmark generation code under
   [build_tools/python/benchmark_suites/iree](/build_tools/python/benchmark_suites/iree).
2. Follow
   [tests/e2e/test_artifacts](https://github.com/openxla/iree/tree/main/tests/e2e/test_artifacts)
   to regenerate the cmake files.
3. Rebuild the benchmark suite.
4. Make sure to export the new benchmark configs again before running the
   benchmarks.

## Fetching Benchmark Artifacts from CI

##### 1. Find the corresponding CI workflow run
On the commit of a benchmark run, you can find the list of the workflow jobs by
clicking the green check mark. Click the `Details` of job
`build_e2e_test_artifacts`:
TODO

##### 2. Find the GCS directory of benchmark artifacts
On the job detail page, expand the step Uploading e2 test artifacts, you will
see a bunch of lines like:
```
Copying file://build-e2e-test-artifacts/e2e_test_artifacts/iree_MobileBertSquad_fp32_module_fdff4caa105318036534bd28b76a6fe34e6e2412752c1a000f50fafe7f01ef07/module.vmfb to gs://iree-github-actions-postsubmit-artifacts/4360950546/1/e2e-test-artifacts/iree_MobileBertSquad_fp32_module_fdff4caa105318036534bd28b76a6fe34e6e2412752c1a000f50fafe7f01ef07/module.vmfb
...
```
The URL `gs://iree-github-actions-...-artifacts/.../.../e2e-test-artifacts/` is
the GCS directory of benchmark artifacts. You can use `gcloud` tool to list the
contents:
```sh
gcloud storage ls gs://iree-github-actions-postsubmit-artifacts/4360950546/1/e2e-test-artifacts
```
It has the same directory structure as your local `"$IREE_BUILD_DIR/e2e_test_artifacts"`

# IREE Benchmark Suites Tool (Legacy)

**For working with the new benchmark suite, see [IREE New Benchmark Suite](#iree-new-benchmark-suite)**

This directory contains the tools to run IREE benchmark suites and generate
reports. More information about benchmark suites can be found [here](/benchmarks/README.md).

## Benchmark Tools

Currently we have `run_benchmarks_on_android.py` and
`run_benchmarks_on_linux.py` scripts to run benchmark suites on Android devices
(with `adb`) and Linux machines.

The available arguments can be shown with `--help`. Some common usages are
listed below. Here we assume:

```sh
IREE_BUILD_DIR="/path/to/IREE build root dir". It should contain the "benchmark_suites" directory built with the target "iree-benchmark-suites".

IREE_NORMAL_TOOL_DIR="/path/to/IREE tool dir". It is usually "$IREE_BUILD_DIR/tools".

IREE_TRACED_TOOL_DIR="/path/to/IREE tool dir built with IREE_ENABLE_RUNTIME_TRACING=ON".
```

See details about `IREE_ENABLE_RUNTIME_TRACING` [here](/docs/developers/developing_iree/profiling_with_tracy.md).

**Run all benchmarks**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --output=results.json $IREE_BUILD_DIR
```

**Run all benchmarks and perform the Tracy captures**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --traced_benchmark_tool_dir=$IREE_TRACED_TOOL_DIR \
  --trace_capture_tool=/path/to/iree-tracy-capture \
  --capture_tarball=captured_tracy_files.tar.gz
  --output=results.json $IREE_BUILD_DIR
```

**Run selected benchmarks with the filters**
```sh
./run_benchmarks_on_linux.py \
  --normal_benchmark_tool_dir=$IREE_NORMAL_TOOL_DIR \
  --model_name_regex="MobileBertSquad" \
  --driver_filter_regex="local-task" \
  --mode_regex="4-threads" \
  --output=results.json $IREE_BUILD_DIR
```

**Collect compilation statistics**

See [here](/benchmarks/README.md#collect-compile-stats) for additional build
steps to enable compilation statistics collection.
```sh
./collect_compilation_statistics.py \
  legacy \
  --output "compile-stats.json" \
  "${IREE_BUILD_DIR}"
```

## Generating Benchmark Report

The tools here are mainly designed for benchmark automation pipelines.
The `post_benchmarks_as_pr_comment.py` and `upload_benchmarks_to_dashboard.py`
scripts are used to upload and post reports to pull requests or the
[dashboard](https://perf.iree.dev/).

If you want to generate a comparison report locally, you can use
`diff_local_benchmarks.py` script to compare two result json files and generate
the report. For example:

```sh
./diff_local_benchmarks.py --base before.json --target after.json > report.md
```

An example that compares compilation statistics:

```sh
./diff_local_benchmarks.py \
  --base-compile-stats "compile-stats-before.json" \
  --target-compile-stats "compile-stats-after.json" \
  > report.md
```
